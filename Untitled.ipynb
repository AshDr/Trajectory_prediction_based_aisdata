{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 173,
   "id": "9f9471e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import copy\n",
    "\n",
    "import torch\n",
    "from torch import nn\n",
    "from torch.utils.data import Dataset, DataLoader,TensorDataset\n",
    "import os\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from torch import tensor\n",
    "from pandas.core.frame import DataFrame\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.metrics import mean_squared_error,mean_absolute_error\n",
    "from sklearn.utils import shuffle\n",
    "import scipy as sp\n",
    "from sklearn.cluster import DBSCAN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 174,
   "outputs": [],
   "source": [
    "def normalization(data,label):   # data[n,10,4] label[n,2]\n",
    "    mm_x = MinMaxScaler()\n",
    "    mm_y = MinMaxScaler()\n",
    "    seq_len = data.shape[1]\n",
    "    feature_num = data.shape[2]\n",
    "    data = data.reshape(-1,feature_num)\n",
    "    data = mm_x.fit_transform(data)\n",
    "    data = data.reshape(-1,seq_len,feature_num)\n",
    "    label = mm_y.fit_transform(label)\n",
    "    return data,label,mm_y"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 175,
   "outputs": [],
   "source": [
    "def dbscan_predict(dbscan_model, X_new, metric=sp.spatial.distance.euclidean):\n",
    "    # Result is noise by default\n",
    "    y_new = np.ones(shape=len(X_new), dtype=int) * -1\n",
    "    # Iterate all input samples for a label\n",
    "    for j, x_new in enumerate(X_new):\n",
    "        # Find a core sample closer than EPS\n",
    "        for i, x_core in enumerate(dbscan_model.components_):\n",
    "            if metric(x_new, x_core) < dbscan_model.eps:\n",
    "                # Assign label of x_core to x_new\n",
    "                y_new[j] = dbscan_model.labels_[dbscan_model.core_sample_indices_[i]]\n",
    "                break\n",
    "\n",
    "    return y_new"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 176,
   "id": "6b686adb",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "class Net(nn.Module):\n",
    "    def __init__(self, feature_size, size_hidden,out_size,num_layers=1,dropout=0):\n",
    "        super().__init__()\n",
    "        self.feature_size = feature_size\n",
    "        self.size_hidden = size_hidden\n",
    "        self.num_layers = num_layers\n",
    "        self.num_direction = 1  # 单向lstm\n",
    "        self.rnn = nn.LSTM(input_size=feature_size, hidden_size=size_hidden, num_layers=num_layers,dropout=dropout, batch_first=True)\n",
    "        self.out = nn.Linear(size_hidden, out_size)\n",
    "        self.dropout=nn.Dropout(p=dropout)\n",
    "    def forward(self, input):\n",
    "        batch_size,seq_len=input.size()[0],input.size()[1]\n",
    "        # print(f\"batch_size:{batch_size}\",f\"seq_len:{seq_len}\")\n",
    "        h_0 = torch.rand(self.num_direction*self.num_layers,input.size(0),self.size_hidden)\n",
    "        h_0=h_0.cuda()\n",
    "        c_0 = torch.rand(self.num_direction*self.num_layers,input.size(0),self.size_hidden)\n",
    "        c_0=c_0.cuda()\n",
    "        # print(f\"h_0 shape:{h_0.shape}\")\n",
    "        output, _ = self.rnn(input,(h_0,c_0))\n",
    "        # output=self.dropout(output)    # 不加dropout效果会好不少\n",
    "        pred = self.out(output)   # [32,10,2]\n",
    "        pred = pred[:,-1,:]   # -> [32,2]?\n",
    "        # print(f\"pred_shape:{pred.shape}\")\n",
    "        return pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 177,
   "id": "03752325",
   "metadata": {},
   "outputs": [],
   "source": [
    "# useful_tag = ['X', 'Y', 'SOG', 'COG', 'Heading']\n",
    "cluster_tag = ['MMSI','LAT', 'LON','SOG','COG']\n",
    "useful_tag = ['LAT', 'LON','SOG']\n",
    "# predict_tag = ['X', 'Y']\n",
    "# predict_tag = ['LAT', 'LON']\n",
    "predict_tag = ['LAT', 'LON','SOG']\n",
    "len_topredict = 5\n",
    "features = ['MMSI', 'BaseDateTime', 'LAT', 'LON', 'SOG', 'COG', 'Heading', 'Status']\n",
    "data_file_root_path = './data/path_data/'\n",
    "ratio = 0.8\n",
    "feature_size=len(useful_tag)\n",
    "batch_size=32 # 最好是2的次幂\n",
    "hidden_size = 256\n",
    "output_size=len(predict_tag)\n",
    "drop_out=0.3\n",
    "lr = 0.01"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 178,
   "id": "eb814964",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MyDataSet(Dataset):\n",
    "    def __init__(self, df: DataFrame):\n",
    "        self.x_data,self.y_data = df[cluster_tag].astype('float32').values,df[cluster_tag].astype('float32').values\n",
    "        self.length = len(self.x_data)\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        return self.x_data[index], self.y_data[index]\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.length"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 179,
   "outputs": [],
   "source": [
    "def norm_for_cluster(X):\n",
    "    f = MinMaxScaler()\n",
    "    data = f.fit_transform(X)\n",
    "    return data"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 180,
   "outputs": [],
   "source": [
    "def get_cluster(Multidimensional_Points):\n",
    "    print(Multidimensional_Points[:5])\n",
    "    cluster = DBSCAN(eps=0.02,min_samples=7)\n",
    "    cluster.fit(np.array(Multidimensional_Points))\n",
    "    labels = cluster.labels_\n",
    "    return labels\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 181,
   "id": "9ada6570",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "199\n",
      "(20231, 5, 4)\n",
      "(20231, 4)\n",
      "[[0.30810362 0.45527586 0.57463074 0.05474096 0.78444445]\n",
      " [0.30810362 0.45527682 0.57459843 0.05767351 0.7372222 ]\n",
      " [0.30810362 0.45527527 0.57456064 0.05474096 0.73444444]\n",
      " [0.30810362 0.45527318 0.57452846 0.03812317 0.74333334]\n",
      " [0.30810362 0.45527133 0.574515   0.00879765 0.69111115]]\n"
     ]
    }
   ],
   "source": [
    "X = []\n",
    "y = []\n",
    "for_cluster_X = []\n",
    "num = 0\n",
    "for i in range(1000):\n",
    "    data_file_path = data_file_root_path + 'id' + str(i) + '/'\n",
    "    \"\"\"\n",
    "        还要做一步归一化\n",
    "    \"\"\"\n",
    "    file_lst = os.listdir(data_file_path)\n",
    "    for filename in file_lst:\n",
    "        path = data_file_path + filename\n",
    "        df = pd.read_csv(path)\n",
    "        if len(y) < 20000:\n",
    "            num += 1\n",
    "            dataset = MyDataSet(df)\n",
    "            for j in range(len_topredict, len(dataset)):\n",
    "                t1, t2,t3 = dataset.x_data[j - len_topredict:j,1:4], dataset.y_data[j,1:4],dataset.x_data[j-len_topredict]   # 改预测的feature的话要改这里\n",
    "                X.append(t1)\n",
    "                y.append(t2)\n",
    "                for_cluster_X.append(t3)\n",
    "            # train_loader = DataLoader(dataset=dataset, batch_size=3)  # 这里batch_size就是一次从一个轨迹中取几个点\n",
    "# 109条轨迹\n",
    "print(num)\n",
    "X,y,for_cluster_X = np.array(X),np.array(y),np.array(for_cluster_X)\n",
    "print(X.shape)\n",
    "print(y.shape)\n",
    "X,y,mm_y = normalization(X,y)\n",
    "labels = get_cluster(norm_for_cluster(for_cluster_X)) #对整个数据进行聚类"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 182,
   "id": "bb6ecfc8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_data(X,y,z,split_ratio):\n",
    "    train_size=int(len(y)*split_ratio)\n",
    "    # X_data = torch.Tensor(np.array(X))\n",
    "    # y_data = torch.Tensor(np.array(y))\n",
    "    X_train = torch.Tensor(np.array(X[0:train_size]))\n",
    "    y_train = torch.Tensor(np.array(y[0:train_size]))\n",
    "    z_train = np.array(z[0:train_size])\n",
    "    X_test = torch.Tensor(np.array(X[train_size:]))\n",
    "    y_test = torch.Tensor(np.array(y[train_size:]))\n",
    "    z_test = np.array(z[train_size:])\n",
    "    return X_train,y_train,z_train,X_test,y_test,z_test\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 183,
   "id": "bad12100",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_dataset(X_train,y_train,X_test,y_test):\n",
    "    train_dataset = TensorDataset(X_train,y_train)\n",
    "    test_dataset = TensorDataset(X_test,y_test)\n",
    "    return train_dataset,test_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 184,
   "outputs": [],
   "source": [
    "def train(net, train_iter, loss, epochs, lr):\n",
    "    trainer = torch.optim.Adam(net.parameters(), lr=lr)\n",
    "    iter = 0\n",
    "    for epoch in range(epochs):\n",
    "        for i,(X, y) in enumerate(train_iter):\n",
    "            X,y = shuffle(X,y,random_state=131)\n",
    "            X = X.cuda()\n",
    "            y = y.cuda()\n",
    "            # print(X.shape,y.shape)  #  X [32,10,5]  y [32,2]\n",
    "            trainer.zero_grad()\n",
    "            l = loss(net(X), y)\n",
    "            iter += 1\n",
    "            if iter % 100 == 0:\n",
    "                print(f'iter: {iter}', f'loss：{l.sum()}')\n",
    "            l.sum().backward()\n",
    "            trainer.step()"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 185,
   "outputs": [],
   "source": [
    "epochs = 20\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "nets = list()\n",
    "label_set = set(labels)\n",
    "cluster_num =  len(label_set) if -1 not in label_set else len(label_set)-1"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 186,
   "outputs": [],
   "source": [
    "def test(net,X,y):\n",
    "    X = X.cuda()\n",
    "    y = y.cuda()\n",
    "    net.eval()\n",
    "    predict = net(X)\n",
    "    predict = predict.data.cpu().numpy()\n",
    "    # y_data_plot = np.reshape(y_data_plot,(-1,1))\n",
    "    predict = mm_y.inverse_transform(predict)\n",
    "    y_data_plot = mm_y.inverse_transform(y.data.cpu().numpy())\n",
    "    print(y_data_plot[:20])\n",
    "    print(predict[:20])\n",
    "    # plt.plot(y_data_plot)\n",
    "    # plt.plot(predict)\n",
    "    # plt.legend(('real','predict'), fontsize='15')\n",
    "    # plt.show()\n",
    "    print(f'LAT:  mean_absolute_error: {mean_absolute_error(y_data_plot[:,0],predict[:,0])}, mean_squared_error: {mean_squared_error(y_data_plot[:,0],predict[:,0])}')\n",
    "    print(f'LON:  mean_absolute_error: {mean_absolute_error(y_data_plot[:,1],predict[:,1])}, mean_squared_error: {mean_squared_error(y_data_plot[:,1],predict[:,1])}')\n",
    "    print(f'SOG:  mean_absolute_error: {mean_absolute_error(y_data_plot[:,2],predict[:,2])}, mean_squared_error: {mean_squared_error(y_data_plot[:,2],predict[:,2])}')\n",
    "    # print(f'COG:  mean_absolute_error: {mean_absolute_error(y_data_plot[:,3],predict[:,3])}, mean_squared_error: {mean_squared_error(y_data_plot[:,3],predict[:,3])}')\n",
    "    # print(f'Heading:  mean_absolute_error: {mean_absolute_error(y_data_plot[:,4],predict[:,4])}, mean_squared_error: {mean_squared_error(y_data_plot[:,4],predict[:,4])}')"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 187,
   "outputs": [],
   "source": [
    "class SmallNet:\n",
    "    def __init__(self,train_iter,test_data_X,test_data_y):\n",
    "        self.device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "        self.net=Net(feature_size=feature_size,size_hidden=hidden_size,out_size=output_size,dropout=drop_out,num_layers=1).to(self.device)\n",
    "        self.loss=nn.MSELoss(reduction='none').to(self.device)\n",
    "        self.train_iter=train_iter\n",
    "        self.test_data_X=torch.stack(test_data_X)\n",
    "        self.test_data_y=torch.stack(test_data_y)\n",
    "    def train(self):\n",
    "        print(\"number of training batch: \",len(self.train_iter))\n",
    "        print(\"Training----\")\n",
    "        train(self.net,self.train_iter,self.loss,epochs,lr)\n",
    "        print(\"Train Done----\")\n",
    "    def test(self):\n",
    "        print(\"Testing----\")\n",
    "        test(self.net,self.test_data_X,self.test_data_y)\n",
    "        print(\"Test Done----\")"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 188,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "unuse train_data:757 , unuse test_data:190\n",
      "number of training batch:  143\n",
      "Training----\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\ashdr\\miniconda3\\lib\\site-packages\\torch\\nn\\modules\\rnn.py:67: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.3 and num_layers=1\n",
      "  warnings.warn(\"dropout option adds dropout after all but last \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iter: 100 loss：0.0838337242603302\n",
      "iter: 200 loss：0.1395207643508911\n",
      "iter: 300 loss：0.24567347764968872\n",
      "iter: 400 loss：0.18065668642520905\n",
      "iter: 500 loss：0.2847444713115692\n",
      "iter: 600 loss：1.3895087242126465\n",
      "iter: 700 loss：0.1973751336336136\n",
      "iter: 800 loss：0.18649566173553467\n",
      "iter: 900 loss：0.5717215538024902\n",
      "iter: 1000 loss：1.0600852966308594\n",
      "iter: 1100 loss：1.212276816368103\n",
      "iter: 1200 loss：0.10978655517101288\n",
      "iter: 1300 loss：0.05253786966204643\n",
      "iter: 1400 loss：0.18154768645763397\n",
      "iter: 1500 loss：0.06481243669986725\n",
      "iter: 1600 loss：0.1805438995361328\n",
      "iter: 1700 loss：0.48834413290023804\n",
      "iter: 1800 loss：1.6129734516143799\n",
      "iter: 1900 loss：1.0272668600082397\n",
      "iter: 2000 loss：0.1261216700077057\n",
      "iter: 2100 loss：0.687880277633667\n",
      "iter: 2200 loss：1.1843634843826294\n",
      "iter: 2300 loss：0.11492057144641876\n",
      "iter: 2400 loss：0.746954619884491\n",
      "iter: 2500 loss：0.6452222466468811\n",
      "iter: 2600 loss：0.5478875637054443\n",
      "iter: 2700 loss：0.28424301743507385\n",
      "iter: 2800 loss：0.22517603635787964\n",
      "Train Done----\n",
      "Testing----\n",
      "[[ 30.05869   -93.58867     6.2        88.9      ]\n",
      " [ 30.31783   -91.17708     0.9       307.7      ]\n",
      " [ 29.90625   -92.54559     6.3        99.69999  ]\n",
      " [ 30.19211   -91.16554     7.        166.5      ]\n",
      " [ 29.65215   -91.26741     3.7999997  87.1      ]\n",
      " [ 29.53542   -90.77159     3.6        92.99999  ]\n",
      " [ 30.28266   -91.31585     6.1       203.9      ]\n",
      " [ 29.93972   -90.21919     7.6999993 334.4      ]\n",
      " [ 29.79528   -92.10385     5.9        71.3      ]\n",
      " [ 30.05277   -90.57773     6.5       266.2      ]\n",
      " [ 30.42773   -91.20158     5.7        12.4      ]\n",
      " [ 30.12905   -91.3222      3.3       352.2      ]\n",
      " [ 29.69942   -91.51395     3.        116.09999  ]\n",
      " [ 30.05935   -93.36628     5.7        88.6      ]\n",
      " [ 30.09212   -93.29946     4.        129.2      ]\n",
      " [ 29.95484   -92.86122     5.7        69.8      ]\n",
      " [ 29.93601   -93.11526     6.4        79.5      ]\n",
      " [ 29.95191   -92.87018     6.4        71.1      ]\n",
      " [ 29.93597   -93.11427     5.2        89.8      ]\n",
      " [ 29.95238   -92.86908     5.4        67.9      ]]\n",
      "[[ 30.145704  -92.56234     6.386628   89.444565 ]\n",
      " [ 30.219744  -90.55481     1.3521923 295.3182   ]\n",
      " [ 30.145123  -91.94067     6.430102   99.97731  ]\n",
      " [ 30.154016  -91.03609     6.86924   185.41351  ]\n",
      " [ 30.083721  -91.00217     3.7889102  99.07382  ]\n",
      " [ 30.079893  -90.775856    3.9914203 103.32785  ]\n",
      " [ 30.186588  -91.05012     5.9411435 205.19637  ]\n",
      " [ 30.21507   -90.03798     6.7420444 345.2997   ]\n",
      " [ 30.15711   -91.64459     6.133022   77.78432  ]\n",
      " [ 30.194004  -90.35326     5.6655946 289.7061   ]\n",
      " [ 30.403181  -90.42021     6.0473065  47.93436  ]\n",
      " [ 30.255154  -90.585724    3.3617852 336.4404   ]\n",
      " [ 30.08646   -91.09468     3.274875  127.88281  ]\n",
      " [ 30.157274  -92.3847      5.9521413  90.574    ]\n",
      " [ 30.129099  -92.11006     4.179942  149.60675  ]\n",
      " [ 30.15546   -92.06596     5.9081683  77.628296 ]\n",
      " [ 30.144278  -92.32057     6.8511314  86.047516 ]\n",
      " [ 30.181015  -92.15478     6.638799   75.46335  ]\n",
      " [ 30.14444   -92.241905    5.489352   88.72824  ]\n",
      " [ 30.197508  -92.024864    5.8455434  72.427086 ]]\n",
      "LAT:  mean_absolute_error: 0.27690738439559937, mean_squared_error: 0.10486304014921188\n",
      "LON:  mean_absolute_error: 0.6158671975135803, mean_squared_error: 0.6321915984153748\n",
      "SOG:  mean_absolute_error: 0.425063818693161, mean_squared_error: 0.45281559228897095\n",
      "Test Done----\n",
      "number of training batch:  7\n",
      "Training----\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\ashdr\\miniconda3\\lib\\site-packages\\torch\\nn\\modules\\rnn.py:67: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.3 and num_layers=1\n",
      "  warnings.warn(\"dropout option adds dropout after all but last \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iter: 100 loss：2.003721237182617\n",
      "Train Done----\n",
      "Testing----\n",
      "[[ 4.07110214e+01 -7.40295181e+01  7.69999933e+00  1.85100006e+02]\n",
      " [ 4.06890907e+01 -7.40102005e+01  7.29999971e+00  2.01000000e+02]\n",
      " [ 4.06866417e+01 -7.40116577e+01  4.50000000e+00  3.02000027e+01]\n",
      " [ 4.07016792e+01 -7.40189285e+01  6.09999990e+00  1.61100006e+02]\n",
      " [ 4.06813507e+01 -7.40395508e+01  3.00000012e-01  2.08500015e+02]\n",
      " [ 4.06994934e+01 -7.40013809e+01  6.50000000e+00  2.09600021e+02]\n",
      " [ 4.06815109e+01 -7.40396194e+01  4.00000006e-01  2.09899994e+02]\n",
      " [ 4.06659889e+01 -7.40304413e+01  4.90000010e+00  1.05399994e+02]\n",
      " [ 4.07251892e+01 -7.40254288e+01  7.09999990e+00  2.07300018e+02]\n",
      " [ 4.07357712e+01 -7.40154572e+01  5.90000010e+00  4.00000000e+00]\n",
      " [ 4.07550316e+01 -7.40170822e+01  8.89999962e+00  1.96699982e+02]\n",
      " [ 4.06813316e+01 -7.40396881e+01  5.00000000e-01  2.08500015e+02]\n",
      " [ 4.06942482e+01 -7.40347824e+01  9.30000019e+00  1.95800003e+02]\n",
      " [ 4.06815414e+01 -7.40404510e+01  5.00000000e-01  2.28800003e+02]\n",
      " [ 4.06870193e+01 -7.40404434e+01  1.60000002e+00  6.20000000e+01]\n",
      " [ 4.06988297e+01 -7.40078888e+01  6.09999990e+00  2.30199997e+02]\n",
      " [ 4.06812210e+01 -7.40394287e+01  1.20000005e+00  2.06999985e+02]\n",
      " [ 4.06994095e+01 -7.40182800e+01  5.09999990e+00  3.09399994e+02]\n",
      " [ 4.06904716e+01 -7.40371323e+01  6.40000010e+00  2.08300003e+02]\n",
      " [ 4.07022400e+01 -7.40207367e+01  4.09999990e+00  3.15000000e+02]]\n",
      "[[ 40.461304  -74.42809     6.461265  191.41231  ]\n",
      " [ 40.409832  -74.57698     6.1870394 190.96016  ]\n",
      " [ 40.43339   -74.3876      7.0832553 183.52687  ]\n",
      " [ 40.407543  -74.42732     6.404208  190.24142  ]\n",
      " [ 40.71008   -74.08436     5.8792343 194.04158  ]\n",
      " [ 40.43268   -74.50001     6.213314  192.41145  ]\n",
      " [ 40.516098  -74.30917     6.062626  193.4121   ]\n",
      " [ 40.483696  -74.35973     6.4209833 188.56981  ]\n",
      " [ 40.491566  -74.23893     5.9613376 192.5314   ]\n",
      " [ 40.636192  -74.05992     5.920363  200.33339  ]\n",
      " [ 40.526787  -74.20168     6.325381  192.79625  ]\n",
      " [ 40.601547  -74.220665    5.9224586 194.1233   ]\n",
      " [ 40.54467   -74.26126     6.24717   191.84753  ]\n",
      " [ 40.539223  -74.28052     5.796233  194.66992  ]\n",
      " [ 40.459305  -74.39859     6.3089952 184.73665  ]\n",
      " [ 40.48586   -74.367325    6.004164  193.81633  ]\n",
      " [ 40.530025  -74.375946    5.8342357 192.94427  ]\n",
      " [ 40.565456  -74.33637     5.896908  197.3108   ]\n",
      " [ 40.477386  -74.504654    6.08601   192.16843  ]\n",
      " [ 40.53095   -74.267555    5.7450867 198.42308  ]]\n",
      "LAT:  mean_absolute_error: 0.18937407433986664, mean_squared_error: 0.04157504811882973\n",
      "LON:  mean_absolute_error: 0.29957115650177, mean_squared_error: 0.1075206845998764\n",
      "SOG:  mean_absolute_error: 2.198272228240967, mean_squared_error: 7.966034412384033\n",
      "Test Done----\n",
      "number of training batch:  6\n",
      "Training----\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\ashdr\\miniconda3\\lib\\site-packages\\torch\\nn\\modules\\rnn.py:67: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.3 and num_layers=1\n",
      "  warnings.warn(\"dropout option adds dropout after all but last \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iter: 100 loss：1.625928282737732\n",
      "Train Done----\n",
      "Testing----\n",
      "[[ 40.76776   -74.00616     5.7        30.3      ]\n",
      " [ 40.70515   -73.99351    10.         71.9      ]\n",
      " [ 40.68719   -74.03613     0.9       200.3      ]\n",
      " [ 40.68402   -74.0416      1.2       204.5      ]\n",
      " [ 40.72926   -74.01735     2.6         6.2      ]\n",
      " [ 40.73073   -74.01468     2.4        64.5      ]\n",
      " [ 40.68798   -74.01017     5.9        33.3      ]\n",
      " [ 40.67056   -74.02239     5.8        11.599999 ]\n",
      " [ 40.75393   -74.01437     6.1       195.8      ]\n",
      " [ 40.716     -74.0194      3.          9.4      ]\n",
      " [ 40.75086   -74.01352     5.9        23.7      ]\n",
      " [ 40.7186    -74.01896     2.9        10.1      ]\n",
      " [ 40.74267   -74.01645     5.8         6.2999997]\n",
      " [ 40.74496   -74.01819     6.5       199.30002  ]\n",
      " [ 40.714603  -74.02574     7.3999996 231.3      ]\n",
      " [ 40.69858   -74.00769     6.        260.1      ]\n",
      " [ 40.70032   -74.01812     7.2       340.8      ]\n",
      " [ 40.71739   -74.01919     2.9         7.3000007]\n",
      " [ 40.71238   -74.02729     8.        201.3      ]\n",
      " [ 40.7122    -74.01884     5.8        14.       ]]\n",
      "[[ 40.591743  -74.57379     6.4986243  75.71497  ]\n",
      " [ 40.358913  -74.836365    6.263389   78.021385 ]\n",
      " [ 41.413548  -72.17601     3.3695726 205.51698  ]\n",
      " [ 39.652912  -76.18066     5.1309342  68.999596 ]\n",
      " [ 40.30315   -74.93558     6.347963   63.393005 ]\n",
      " [ 40.415077  -74.95468     6.4032393  65.82011  ]\n",
      " [ 40.380547  -74.743385    5.981247   84.22361  ]\n",
      " [ 40.341442  -75.04378     6.3232746  64.717094 ]\n",
      " [ 40.404396  -74.82179     6.1879525  68.78643  ]\n",
      " [ 40.37983   -74.957535    6.1250677  62.462456 ]\n",
      " [ 40.448616  -74.9099      6.2462816  68.42379  ]\n",
      " [ 40.30359   -75.0209      6.2317905  63.298195 ]\n",
      " [ 40.385433  -74.94376     6.189246   62.148483 ]\n",
      " [ 41.243202  -72.49364     3.4107208 197.22758  ]\n",
      " [ 41.318756  -72.106804    1.9367813 240.55551  ]\n",
      " [ 42.700806  -69.6128      1.9387623 244.98709  ]\n",
      " [ 42.058926  -70.58797     2.054303  293.0469   ]\n",
      " [ 40.50808   -74.827614    6.349736   63.86056  ]\n",
      " [ 41.18955   -72.19471     2.1235476 228.16423  ]\n",
      " [ 40.44903   -74.913055    6.478718   63.817368 ]]\n",
      "LAT:  mean_absolute_error: 0.7669105529785156, mean_squared_error: 1.1300654411315918\n",
      "LON:  mean_absolute_error: 1.873766303062439, mean_squared_error: 6.002933025360107\n",
      "SOG:  mean_absolute_error: 3.1016452312469482, mean_squared_error: 14.374146461486816\n",
      "Test Done----\n",
      "number of training batch:  59\n",
      "Training----\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\ashdr\\miniconda3\\lib\\site-packages\\torch\\nn\\modules\\rnn.py:67: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.3 and num_layers=1\n",
      "  warnings.warn(\"dropout option adds dropout after all but last \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iter: 100 loss：1.530205249786377\n",
      "iter: 200 loss：1.862841248512268\n",
      "iter: 300 loss：2.043771266937256\n",
      "iter: 400 loss：0.7231346368789673\n",
      "iter: 500 loss：1.1067371368408203\n",
      "iter: 600 loss：1.4071898460388184\n",
      "iter: 700 loss：1.6303383111953735\n",
      "iter: 800 loss：1.3365509510040283\n",
      "iter: 900 loss：1.7228069305419922\n",
      "iter: 1000 loss：1.700258493423462\n",
      "iter: 1100 loss：0.6264661550521851\n",
      "Train Done----\n",
      "Testing----\n",
      "[[ 3.99057312e+01 -7.40181503e+01  2.70000005e+00  3.30100006e+02]\n",
      " [ 3.99005203e+01 -7.40172806e+01  1.10000002e+00  1.51600006e+02]\n",
      " [ 3.99295807e+01 -7.40234299e+01  3.00000012e-01  1.15899994e+02]\n",
      " [ 3.99178085e+01 -7.40315323e+01  1.00000000e+00  3.07999992e+01]\n",
      " [ 3.99287300e+01 -7.40260620e+01  5.00000000e-01  2.37699997e+02]\n",
      " [ 3.99275284e+01 -7.40209274e+01  3.00000012e-01  2.06399994e+02]\n",
      " [ 3.99252586e+01 -7.40229416e+01  3.00000012e-01  2.14599991e+02]\n",
      " [ 3.99145889e+01 -7.40188065e+01  1.20000005e+00  1.41600006e+02]\n",
      " [ 3.99348907e+01 -7.51415482e+01  1.00000001e-01  1.91800003e+02]\n",
      " [ 3.99262505e+01 -7.40212936e+01  2.00000003e-01  2.24300003e+02]\n",
      " [ 3.99262085e+01 -7.40213470e+01  5.00000000e-01  2.13000000e+02]\n",
      " [ 3.99100609e+01 -7.40642166e+01  4.00000006e-01  3.28399994e+02]\n",
      " [ 3.99198608e+01 -7.40290375e+01  8.00000012e-01  1.69800003e+02]\n",
      " [ 3.99348488e+01 -7.51415100e+01  1.00000001e-01  1.95199982e+02]\n",
      " [ 3.99167709e+01 -7.40322266e+01  6.99999988e-01  2.86799988e+02]\n",
      " [ 3.99293594e+01 -7.40225525e+01  4.00000006e-01  1.04599998e+02]\n",
      " [ 3.99264908e+01 -7.40211411e+01  3.00000012e-01  1.75000000e+01]\n",
      " [ 3.99124298e+01 -7.40632401e+01  5.40000010e+00  8.85999985e+01]\n",
      " [ 3.99292488e+01 -7.40253601e+01  4.00000006e-01  3.06799988e+02]\n",
      " [ 3.99349098e+01 -7.51415634e+01  1.00000001e-01  1.87600006e+02]]\n",
      "[[ 3.9969315e+01 -7.4019516e+01  2.7210960e+00  3.2437827e+02]\n",
      " [ 3.9984344e+01 -7.4193459e+01  7.6883656e-01  1.6059558e+02]\n",
      " [ 4.0133888e+01 -7.4028740e+01  3.8629994e-01  1.4698814e+02]\n",
      " [ 4.0104153e+01 -7.3764427e+01  1.3359039e+00  1.2621865e+02]\n",
      " [ 4.0262764e+01 -7.3428123e+01  3.7692267e-01  1.5208034e+02]\n",
      " [ 3.9949795e+01 -7.4112694e+01  4.8692942e-01  1.9760788e+02]\n",
      " [ 4.0020042e+01 -7.3865067e+01  6.7623681e-01  2.1778383e+02]\n",
      " [ 3.9950272e+01 -7.4213028e+01  9.6117973e-01  1.6044539e+02]\n",
      " [ 4.0064594e+01 -7.4169327e+01  1.0107669e-01  1.7502502e+02]\n",
      " [ 4.0011715e+01 -7.4141518e+01  5.2239072e-01  1.9525520e+02]\n",
      " [ 3.9993877e+01 -7.4416931e+01  5.8705318e-01  2.1910400e+02]\n",
      " [ 4.0136765e+01 -7.4057266e+01  3.2250047e-01  1.4911002e+02]\n",
      " [ 3.9724308e+01 -7.4061829e+01  1.2218441e+00  2.1079094e+02]\n",
      " [ 4.0014217e+01 -7.4231300e+01  1.5906978e-01  1.6743654e+02]\n",
      " [ 4.0052052e+01 -7.4111740e+01  1.3227243e+00  2.8833331e+02]\n",
      " [ 4.0077160e+01 -7.4024551e+01  5.0453079e-01  1.3851634e+02]\n",
      " [ 4.0052685e+01 -7.4168411e+01  1.6092192e-01  1.7519771e+02]\n",
      " [ 3.9338009e+01 -7.4620667e+01  2.4803939e-01  1.6932315e+02]\n",
      " [ 4.0249702e+01 -7.3970367e+01  7.2218949e-01  2.2040108e+02]\n",
      " [ 3.9993870e+01 -7.4135010e+01  2.4824823e-01  1.8169789e+02]]\n",
      "LAT:  mean_absolute_error: 0.1126495823264122, mean_squared_error: 0.02143094502389431\n",
      "LON:  mean_absolute_error: 0.3253437578678131, mean_squared_error: 0.20864063501358032\n",
      "SOG:  mean_absolute_error: 0.35697874426841736, mean_squared_error: 0.4125979244709015\n",
      "Test Done----\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\ashdr\\miniconda3\\lib\\site-packages\\torch\\nn\\modules\\rnn.py:67: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.3 and num_layers=1\n",
      "  warnings.warn(\"dropout option adds dropout after all but last \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "number of training batch:  5\n",
      "Training----\n",
      "iter: 100 loss：0.00823121052235365\n",
      "Train Done----\n",
      "Testing----\n",
      "[[ 27.032043 -90.38795    8.848148 115.34013 ]\n",
      " [ 27.205002 -90.851715   8.5      112.06667 ]\n",
      " [ 27.6903   -92.16282    9.       113.3     ]\n",
      " [ 27.5917   -91.9118     9.2      111.6     ]\n",
      " [ 27.54806  -91.78894    8.6      112.5     ]\n",
      " [ 27.6146   -91.97415    8.8      114.3     ]\n",
      " [ 26.942745 -90.14004    8.9      113.984955]\n",
      " [ 27.084732 -90.517525   8.76875  113.90909 ]\n",
      " [ 27.68009  -92.13676    8.9      113.2     ]\n",
      " [ 27.53928  -91.76462    8.4      110.8     ]\n",
      " [ 27.475452 -91.58399    8.6      110.19999 ]\n",
      " [ 27.69674  -92.18159    9.       110.7     ]\n",
      " [ 27.43654  -91.45984    8.3      105.7     ]\n",
      " [ 27.195713 -90.82681    8.5      112.933334]\n",
      " [ 27.71968  -92.24833    9.       112.3     ]\n",
      " [ 27.221392 -90.89356    8.55     112.03893 ]\n",
      " [ 27.71839  -92.24477    9.1      110.1     ]\n",
      " [ 27.104992 -90.573      8.5      112.4     ]\n",
      " [ 27.16479  -90.73642    8.5      109.399994]\n",
      " [ 27.67634  -92.12632    8.9      111.700005]]\n",
      "[[ 27.45942   -91.54806     8.94515   112.94192  ]\n",
      " [ 27.468054  -91.491196    9.01913   112.50128  ]\n",
      " [ 27.402777  -91.785995    8.842932  112.574326 ]\n",
      " [ 27.412378  -91.74632     8.906001  112.62209  ]\n",
      " [ 27.457577  -91.52291     8.954161  112.58207  ]\n",
      " [ 27.478304  -91.534256    8.915582  112.907684 ]\n",
      " [ 27.425184  -91.646065    8.99638   112.431656 ]\n",
      " [ 27.491314  -91.477776    9.0512    112.27624  ]\n",
      " [ 27.527945  -91.47685     9.066096  112.528786 ]\n",
      " [ 27.383324  -91.895546    8.9000845 112.73276  ]\n",
      " [ 27.49111   -91.463264    8.968348  112.33914  ]\n",
      " [ 27.450592  -91.66042     8.940121  112.62442  ]\n",
      " [ 27.522125  -91.43714     9.008883  112.8589   ]\n",
      " [ 27.442148  -91.58327     8.957757  112.787155 ]\n",
      " [ 27.419239  -91.681885    8.980713  112.31908  ]\n",
      " [ 27.502512  -91.504715    9.010888  112.63689  ]\n",
      " [ 27.46601   -91.574936    8.986274  112.50581  ]\n",
      " [ 27.517214  -91.38336     8.979961  112.41034  ]\n",
      " [ 27.45679   -91.66527     8.924731  112.90075  ]\n",
      " [ 27.401821  -91.72783     8.939813  112.70064  ]]\n",
      "LAT:  mean_absolute_error: 0.23115693032741547, mean_squared_error: 0.07046230137348175\n",
      "LON:  mean_absolute_error: 0.5977164506912231, mean_squared_error: 0.5164023637771606\n",
      "SOG:  mean_absolute_error: 0.26762810349464417, mean_squared_error: 0.10405871272087097\n",
      "Test Done----\n",
      "number of training batch:  5\n",
      "Training----\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\ashdr\\miniconda3\\lib\\site-packages\\torch\\nn\\modules\\rnn.py:67: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.3 and num_layers=1\n",
      "  warnings.warn(\"dropout option adds dropout after all but last \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iter: 100 loss：1.3172872066497803\n",
      "Train Done----\n",
      "Testing----\n",
      "[[ 2.9743120e+01 -9.5195679e+01  1.0000000e-01  2.5150000e+02]\n",
      " [ 2.9720600e+01 -9.5247299e+01  6.6999998e+00  2.9879999e+02]\n",
      " [ 2.9743000e+01 -9.5194519e+01  6.6999998e+00  2.4370000e+02]\n",
      " [ 2.9737669e+01 -9.5203423e+01  7.4999995e+00  2.2160001e+02]\n",
      " [ 2.9723619e+01 -9.5250977e+01  6.8000002e+00  3.1629999e+02]\n",
      " [ 2.9724930e+01 -9.5216782e+01  6.5999999e+00  2.5639999e+02]\n",
      " [ 2.9724768e+01 -9.5231033e+01  6.6999998e+00  2.4700002e+02]\n",
      " [ 2.9745050e+01 -9.5190117e+01  6.6999998e+00  2.4970001e+02]\n",
      " [ 2.9749029e+01 -9.5289162e+01  5.5000000e+00  6.0000000e+01]\n",
      " [ 2.9613670e+01 -9.4994431e+01  6.0000000e+00  2.6410001e+02]\n",
      " [ 2.9724800e+01 -9.5218498e+01  5.0000000e+00  2.7200000e+02]\n",
      " [ 2.9763950e+01 -9.5097504e+01  4.6999998e+00  2.7989999e+02]\n",
      " [ 2.9755070e+01 -9.5177254e+01  1.2000000e+00  3.3639999e+02]\n",
      " [ 2.9745220e+01 -9.5188797e+01  3.3000000e+00  2.4970001e+02]\n",
      " [ 2.9746220e+01 -9.5180771e+01  4.8000002e+00  2.6279999e+02]\n",
      " [ 2.9735649e+01 -9.5127373e+01  4.6999998e+00  2.6089999e+02]\n",
      " [ 2.9614300e+01 -9.4981049e+01  7.6999993e+00  2.7120001e+02]\n",
      " [ 2.9752350e+01 -9.5171402e+01  1.0000000e+00  3.5510001e+02]\n",
      " [ 2.9728020e+01 -9.5274620e+01  6.8000002e+00  3.2220001e+02]\n",
      " [ 2.9736931e+01 -9.5122726e+01  4.8000002e+00  2.4590001e+02]]\n",
      "[[ 29.430223  -95.66319     3.4230165 228.7585   ]\n",
      " [ 30.120543  -94.75731     3.1808362 247.41232  ]\n",
      " [ 29.943892  -94.78202     3.4973614 241.91948  ]\n",
      " [ 29.579859  -94.86467     4.262862  235.7001   ]\n",
      " [ 30.596495  -94.13202     2.788616  265.38995  ]\n",
      " [ 29.798477  -95.00786     3.5715306 237.1212   ]\n",
      " [ 30.196526  -95.15106     2.7452226 246.7564   ]\n",
      " [ 29.984144  -94.94762     3.371597  245.66977  ]\n",
      " [ 27.2829    -97.4314      6.0848193 169.4384   ]\n",
      " [ 29.896582  -94.51731     4.087762  245.72035  ]\n",
      " [ 29.830153  -95.005554    3.4704227 239.22217  ]\n",
      " [ 30.073517  -95.09916     2.9375834 243.18385  ]\n",
      " [ 30.452219  -94.5458      2.3522344 260.51575  ]\n",
      " [ 29.797173  -94.95372     3.5090568 240.14969  ]\n",
      " [ 29.867865  -94.8446      3.5897112 242.40324  ]\n",
      " [ 29.55515   -95.44422     3.5337596 230.62592  ]\n",
      " [ 30.03266   -94.84914     3.1840885 246.73949  ]\n",
      " [ 30.174473  -95.05427     2.556839  249.24812  ]\n",
      " [ 30.481472  -94.36403     2.8756936 258.06647  ]\n",
      " [ 29.47558   -94.630936    4.526017  235.0494   ]]\n",
      "LAT:  mean_absolute_error: 0.4382079243659973, mean_squared_error: 0.4191800355911255\n",
      "LON:  mean_absolute_error: 0.46209168434143066, mean_squared_error: 0.40472114086151123\n",
      "SOG:  mean_absolute_error: 2.2846462726593018, mean_squared_error: 7.068862438201904\n",
      "Test Done----\n",
      "number of training batch:  9\n",
      "Training----\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\ashdr\\miniconda3\\lib\\site-packages\\torch\\nn\\modules\\rnn.py:67: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.3 and num_layers=1\n",
      "  warnings.warn(\"dropout option adds dropout after all but last \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iter: 100 loss：0.03664398193359375\n",
      "Train Done----\n",
      "Testing----\n",
      "[[ 30.02863  -88.20493    9.       230.1     ]\n",
      " [ 29.68512  -88.66509    9.4      230.8     ]\n",
      " [ 29.62227  -88.74947    9.8      229.7     ]\n",
      " [ 29.50714  -88.87017   10.       221.3     ]\n",
      " [ 29.74687  -88.58004    9.1      232.8     ]\n",
      " [ 29.42757  -88.9366     9.5      173.      ]\n",
      " [ 29.96128  -88.29375    9.1      227.8     ]\n",
      " [ 28.85905  -88.86724    9.4      199.9     ]\n",
      " [ 29.50953  -88.86772   10.       220.8     ]\n",
      " [ 28.88474  -88.85653    9.5      199.6     ]\n",
      " [ 29.63927  -88.72735    9.8      225.69998 ]\n",
      " [ 29.65785  -88.70385    9.5      227.5     ]\n",
      " [ 29.70445  -88.63832    9.5      231.9     ]\n",
      " [ 29.44781  -88.93015    9.9      213.3     ]\n",
      " [ 29.560148 -88.81613    9.7      222.3     ]\n",
      " [ 30.04835  -88.17887    9.2      229.09999 ]\n",
      " [ 29.85796  -88.43011    9.1      230.2     ]\n",
      " [ 29.49553  -88.88224    9.9      223.2     ]\n",
      " [ 29.624388 -88.74676    9.7      229.49998 ]\n",
      " [ 29.72188  -88.61444    9.3      228.8     ]]\n",
      "[[ 29.477623 -88.81647    9.141464 222.44269 ]\n",
      " [ 29.519445 -88.86718    9.36016  222.35449 ]\n",
      " [ 29.475502 -88.83108    9.032633 222.19438 ]\n",
      " [ 29.469542 -88.84872    9.034237 221.58644 ]\n",
      " [ 29.589434 -88.703674   9.170414 223.63188 ]\n",
      " [ 29.15731  -89.418465   8.779414 216.23705 ]\n",
      " [ 29.523224 -88.78596    9.18918  222.29764 ]\n",
      " [ 29.220135 -89.344765   8.945001 218.425   ]\n",
      " [ 29.495953 -88.87977    9.150358 222.03366 ]\n",
      " [ 29.198256 -89.370575   9.10104  217.91281 ]\n",
      " [ 29.591482 -88.66509    9.287681 223.3361  ]\n",
      " [ 29.466038 -88.81772    9.146519 222.02107 ]\n",
      " [ 29.56912  -88.769264   9.258359 223.6026  ]\n",
      " [ 29.367638 -89.14625    8.949999 219.97446 ]\n",
      " [ 29.38116  -89.07242    9.100749 220.70578 ]\n",
      " [ 29.508656 -88.72444    9.121189 222.95557 ]\n",
      " [ 29.605572 -88.62702    9.246158 223.67628 ]\n",
      " [ 29.458284 -88.777      9.07283  221.86813 ]\n",
      " [ 29.534575 -88.85       9.24236  222.34213 ]\n",
      " [ 29.574318 -88.64729    9.054906 223.89326 ]]\n",
      "LAT:  mean_absolute_error: 0.2701844871044159, mean_squared_error: 0.09946988523006439\n",
      "LON:  mean_absolute_error: 0.2997909486293793, mean_squared_error: 0.1309274584054947\n",
      "SOG:  mean_absolute_error: 0.40524566173553467, mean_squared_error: 0.25886303186416626\n",
      "Test Done----\n",
      "number of training batch:  3\n",
      "Training----\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\ashdr\\miniconda3\\lib\\site-packages\\torch\\nn\\modules\\rnn.py:67: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.3 and num_layers=1\n",
      "  warnings.warn(\"dropout option adds dropout after all but last \"\n",
      "C:\\Users\\ashdr\\miniconda3\\lib\\site-packages\\torch\\nn\\modules\\rnn.py:67: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.3 and num_layers=1\n",
      "  warnings.warn(\"dropout option adds dropout after all but last \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Done----\n",
      "Testing----\n",
      "[[ 29.25069 -88.86848  10.      157.6    ]\n",
      " [ 29.23327 -88.85938   9.9     152.9    ]\n",
      " [ 29.13702 -88.80557   9.8     157.     ]\n",
      " [ 29.14875 -88.81106   9.7     156.9    ]\n",
      " [ 29.08209 -88.78159   9.2     161.6    ]\n",
      " [ 29.3002  -88.89226   9.7     157.8    ]\n",
      " [ 29.19733 -88.83641  10.      149.9    ]\n",
      " [ 29.39619 -88.92982   9.9     167.6    ]\n",
      " [ 29.30316 -88.89369   9.9     157.3    ]\n",
      " [ 29.12542 -88.80008   9.7     157.7    ]\n",
      " [ 29.31508 -88.89931  10.1     156.9    ]\n",
      " [ 29.40555 -88.93215   9.8     168.     ]\n",
      " [ 29.25715 -88.87155  10.2     156.4    ]\n",
      " [ 29.21965 -88.85063   9.9     151.     ]\n",
      " [ 29.18939 -88.83135   9.8     152.1    ]\n",
      " [ 29.41822 -88.93483   9.8     169.4    ]\n",
      " [ 29.3618  -88.91867   9.9     163.     ]\n",
      " [ 29.26603 -88.87583   9.7     157.     ]\n",
      " [ 29.09095 -88.7852    9.2     158.8    ]\n",
      " [ 29.21405 -88.84713   9.7     151.     ]]\n",
      "[[ 29.10843  -89.18654    9.610907 158.60008 ]\n",
      " [ 29.097551 -89.124626   9.375472 158.5663  ]\n",
      " [ 29.113419 -89.22765    9.724914 158.82979 ]\n",
      " [ 29.06834  -89.3748     9.672713 158.15228 ]\n",
      " [ 29.095964 -89.319435   9.920464 158.66705 ]\n",
      " [ 29.070246 -89.43235    9.944917 158.55861 ]\n",
      " [ 29.048685 -89.39785    9.599853 158.15167 ]\n",
      " [ 29.13439  -89.22745    9.70762  159.06342 ]\n",
      " [ 29.130259 -89.17809    9.573644 159.1218  ]\n",
      " [ 29.057596 -89.51017   10.063133 158.48784 ]\n",
      " [ 29.06969  -89.442116   9.664977 158.13501 ]\n",
      " [ 29.128517 -89.20744    9.760531 158.86636 ]\n",
      " [ 29.084715 -89.34699    9.66793  158.59126 ]\n",
      " [ 29.092564 -89.1921     9.394632 158.43001 ]\n",
      " [ 29.087664 -89.22209    9.632413 158.96497 ]\n",
      " [ 29.172136 -89.11296    9.567798 159.12613 ]\n",
      " [ 29.09785  -89.28243    9.623378 158.70775 ]\n",
      " [ 29.098919 -89.29568    9.439288 158.1876  ]\n",
      " [ 29.137854 -89.071526   9.582483 159.1201  ]\n",
      " [ 29.085487 -89.16831    9.564501 158.93202 ]]\n",
      "LAT:  mean_absolute_error: 0.16226691007614136, mean_squared_error: 0.033085815608501434\n",
      "LON:  mean_absolute_error: 0.4034607708454132, mean_squared_error: 0.17914845049381256\n",
      "SOG:  mean_absolute_error: 0.2947196960449219, mean_squared_error: 0.11818478256464005\n",
      "Test Done----\n",
      "number of training batch:  9\n",
      "Training----\n",
      "iter: 100 loss：0.007665849756449461\n",
      "Train Done----\n",
      "Testing----\n",
      "[[ 28.7529   -88.97942    9.1      233.8     ]\n",
      " [ 28.49299  -89.3539     9.       239.00002 ]\n",
      " [ 28.25947  -89.72922    9.5      236.30002 ]\n",
      " [ 28.51468  -89.3134     9.3      238.      ]\n",
      " [ 28.53215  -89.28468    9.3      233.29999 ]\n",
      " [ 28.72809  -89.01379    9.       230.2     ]\n",
      " [ 28.54855  -89.25916    9.2      233.4     ]\n",
      " [ 28.29516  -89.67101    9.5      234.89998 ]\n",
      " [ 28.60847  -89.17575    9.5      230.59999 ]\n",
      " [ 28.68913  -89.0624     9.4      228.2     ]\n",
      " [ 28.124743 -89.93275    9.6      241.70001 ]\n",
      " [ 28.10019  -89.98413    9.9      243.99998 ]\n",
      " [ 28.06232  -90.06429    9.9      243.9     ]\n",
      " [ 28.63837  -89.13306    9.5      233.2     ]\n",
      " [ 28.71479  -89.03128    9.2      229.6     ]\n",
      " [ 28.751013 -88.98203    9.2      231.00002 ]\n",
      " [ 28.0377   -90.12175   10.       246.1     ]\n",
      " [ 28.34443  -89.59921    9.4      234.      ]\n",
      " [ 28.41321  -89.49794    9.5      234.4     ]\n",
      " [ 28.35288  -89.58625    9.5      231.79999 ]]\n",
      "[[ 28.473007 -89.536896   9.380543 234.51913 ]\n",
      " [ 28.463087 -89.46257    9.473999 234.78397 ]\n",
      " [ 28.401735 -89.629974   9.229257 233.79643 ]\n",
      " [ 28.505047 -89.45923    9.399439 235.13531 ]\n",
      " [ 28.443281 -89.49852    9.617692 234.70341 ]\n",
      " [ 28.424171 -89.611244   9.381427 233.80815 ]\n",
      " [ 28.519802 -89.38036    9.628049 235.39963 ]\n",
      " [ 28.485851 -89.5        9.567742 234.70337 ]\n",
      " [ 28.4187   -89.5989     9.23979  234.39992 ]\n",
      " [ 28.541433 -89.39554    9.586393 235.49623 ]\n",
      " [ 28.452515 -89.533966   9.591453 234.58934 ]\n",
      " [ 28.463428 -89.50481    9.580608 234.84029 ]\n",
      " [ 28.435804 -89.46616    9.502723 234.8352  ]\n",
      " [ 28.442314 -89.6333     9.360365 234.19315 ]\n",
      " [ 28.431429 -89.51762    9.399044 234.66908 ]\n",
      " [ 28.517962 -89.432945   9.433821 235.17711 ]\n",
      " [ 28.415623 -89.565895   9.493997 234.03058 ]\n",
      " [ 28.425606 -89.665054   9.352688 233.68483 ]\n",
      " [ 28.457006 -89.564644   9.478513 233.84064 ]\n",
      " [ 28.519749 -89.37192    9.515938 235.43677 ]]\n",
      "LAT:  mean_absolute_error: 0.18768082559108734, mean_squared_error: 0.05140344426035881\n",
      "LON:  mean_absolute_error: 0.3077920377254486, mean_squared_error: 0.1395167112350464\n",
      "SOG:  mean_absolute_error: 0.21880905330181122, mean_squared_error: 0.08141002804040909\n",
      "Test Done----\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\ashdr\\miniconda3\\lib\\site-packages\\torch\\nn\\modules\\rnn.py:67: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.3 and num_layers=1\n",
      "  warnings.warn(\"dropout option adds dropout after all but last \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "number of training batch:  10\n",
      "Training----\n",
      "iter: 100 loss：0.5486551523208618\n",
      "iter: 200 loss：0.7220097184181213\n",
      "Train Done----\n",
      "Testing----\n",
      "[[ 2.8695530e+01 -9.5947273e+01  3.0999999e+00  2.3950000e+02]\n",
      " [ 2.8690750e+01 -9.5957497e+01  1.1000000e+00  2.4420000e+02]\n",
      " [ 2.8754829e+01 -9.5686600e+01  4.5999999e+00  2.4170001e+02]\n",
      " [ 2.8715231e+01 -9.5894783e+01  4.0000001e-01  2.4389999e+02]\n",
      " [ 2.8671869e+01 -9.5998817e+01  1.0000000e-01  2.0289999e+02]\n",
      " [ 2.8664730e+01 -9.6013527e+01  3.3000000e+00  2.4180000e+02]\n",
      " [ 2.8742670e+01 -9.5805931e+01  4.5000000e+00  2.5610001e+02]\n",
      " [ 2.8763248e+01 -9.5670624e+01  4.1999998e+00  2.3510001e+02]\n",
      " [ 2.8668131e+01 -9.6006378e+01  3.7999997e+00  2.4300000e+02]\n",
      " [ 2.8704399e+01 -9.5926949e+01  1.3000000e+00  2.7710001e+02]\n",
      " [ 2.8673830e+01 -9.5994202e+01  2.5999999e+00  2.4270000e+02]\n",
      " [ 2.8676220e+01 -9.5988831e+01  2.4000001e+00  6.4900002e+01]\n",
      " [ 2.8671930e+01 -9.5998283e+01  6.9999999e-01  2.7689999e+02]\n",
      " [ 2.8691050e+01 -9.5956718e+01  1.4000000e+00  2.4960001e+02]\n",
      " [ 2.8726700e+01 -9.5858871e+01  4.5999999e+00  2.4889999e+02]\n",
      " [ 2.8704599e+01 -9.5927116e+01  3.0000001e-01  3.4829999e+02]\n",
      " [ 2.8726130e+01 -9.5860474e+01  4.5000000e+00  2.5000000e+02]\n",
      " [ 2.8745470e+01 -9.5795349e+01  4.5999999e+00  2.5420000e+02]\n",
      " [ 2.8759480e+01 -9.5676750e+01  4.4000001e+00  2.3380000e+02]\n",
      " [ 2.8672279e+01 -9.5997627e+01  1.5000000e+00  2.4320001e+02]]\n",
      "[[ 29.244505  -94.80071     4.81924   219.47179  ]\n",
      " [ 29.190928  -94.9428      4.414312  213.12448  ]\n",
      " [ 29.222927  -94.87631     5.432729  228.33337  ]\n",
      " [ 29.117027  -95.15511     4.492283  217.85033  ]\n",
      " [ 28.77597   -95.65023     4.483218  210.30522  ]\n",
      " [ 29.192106  -94.97093     5.0713625 223.49937  ]\n",
      " [ 29.176634  -94.91364     5.5732293 236.34941  ]\n",
      " [ 29.23688   -94.87054     5.4201694 229.00127  ]\n",
      " [ 29.217342  -94.88774     5.2109303 224.88979  ]\n",
      " [ 29.114195  -95.05173     4.6787786 217.73601  ]\n",
      " [ 29.265425  -94.82235     5.0887947 224.3359   ]\n",
      " [ 28.693325  -95.71207     1.1809385  88.79819  ]\n",
      " [ 29.16052   -95.008484    4.4325895 209.00221  ]\n",
      " [ 29.105537  -94.97864     4.8307414 219.99747  ]\n",
      " [ 29.322954  -94.77367     5.6744604 236.17744  ]\n",
      " [ 28.328074  -96.38561     1.8808764  99.86043  ]\n",
      " [ 29.102177  -94.99444     5.631112  233.76204  ]\n",
      " [ 29.31379   -94.69615     5.5685167 237.29488  ]\n",
      " [ 29.198338  -94.97148     5.1489005 222.69675  ]\n",
      " [ 29.208107  -94.92296     4.8202834 218.68546  ]]\n",
      "LAT:  mean_absolute_error: 0.46040526032447815, mean_squared_error: 0.24016250669956207\n",
      "LON:  mean_absolute_error: 0.9016160368919373, mean_squared_error: 0.8996760249137878\n",
      "SOG:  mean_absolute_error: 2.1969716548919678, mean_squared_error: 6.392619609832764\n",
      "Test Done----\n",
      "number of training batch:  4\n",
      "Training----\n",
      "Train Done----\n",
      "Testing----\n",
      "[[ 16.06956   -67.21344    12.518076  156.83247  ]\n",
      " [ 16.83302   -67.56107    12.6       155.       ]\n",
      " [ 16.6877    -67.49412    12.6       155.       ]\n",
      " [ 16.88097   -67.58235    12.6       157.       ]\n",
      " [ 15.849113  -67.11401    12.400386  156.98001  ]\n",
      " [ 16.58817   -67.45017    12.5       156.       ]\n",
      " [ 16.18365   -67.26533    12.7       156.       ]\n",
      " [ 16.9897    -67.63088    12.5       155.       ]\n",
      " [ 16.8089    -67.55033    12.5       157.       ]\n",
      " [ 14.546199  -66.52429    12.573089  155.       ]\n",
      " [ 14.60679   -66.55174    12.523518  155.       ]\n",
      " [ 16.37243   -67.35217    12.3       157.       ]\n",
      " [ 16.92178   -67.60022    12.5       156.       ]\n",
      " [ 15.5246315 -66.968704   12.407636  156.74261  ]\n",
      " [ 17.01895   -67.64442    12.5       157.       ]\n",
      " [ 16.65465   -67.47955    12.6       157.       ]\n",
      " [ 16.67307   -67.48763    12.5       156.       ]\n",
      " [ 14.398929  -66.45828    12.599082  155.       ]\n",
      " [ 14.849979  -66.664795   12.48585   155.2261   ]\n",
      " [ 16.71957   -67.50962    12.6       154.       ]]\n",
      "[[ 16.019686  -67.30146    12.617953  155.59908  ]\n",
      " [ 16.005007  -67.380005   12.595459  155.02641  ]\n",
      " [ 16.052044  -67.482704   12.572103  154.5053   ]\n",
      " [ 16.113647  -67.06354    12.560461  155.83418  ]\n",
      " [ 15.990458  -67.24815    12.7040615 155.27534  ]\n",
      " [ 16.036379  -67.59638    12.513981  154.6219   ]\n",
      " [ 16.018957  -67.279755   12.662952  155.22394  ]\n",
      " [ 16.08142   -67.162224   12.638543  155.75198  ]\n",
      " [ 16.006813  -67.5156     12.570101  154.43922  ]\n",
      " [ 15.946799  -67.60184    12.490119  154.31871  ]\n",
      " [ 15.867994  -67.62398    12.62007   154.07433  ]\n",
      " [ 16.02904   -67.046776   12.574159  155.34933  ]\n",
      " [ 16.025986  -67.05539    12.589587  155.48708  ]\n",
      " [ 15.985094  -67.477806   12.577156  154.39825  ]\n",
      " [ 16.04259   -67.45682    12.493968  154.6192   ]\n",
      " [ 16.042028  -67.185165   12.784012  155.54204  ]\n",
      " [ 16.056623  -67.56697    12.548255  154.4838   ]\n",
      " [ 16.001818  -67.57203    12.483213  154.45862  ]\n",
      " [ 15.951282  -67.18125    12.7119875 155.46326  ]\n",
      " [ 16.078516  -67.53434    12.481155  154.57425  ]]\n",
      "LAT:  mean_absolute_error: 0.7929096221923828, mean_squared_error: 0.7654438614845276\n",
      "LON:  mean_absolute_error: 0.3767280578613281, mean_squared_error: 0.2809363305568695\n",
      "SOG:  mean_absolute_error: 0.11608485877513885, mean_squared_error: 0.023548761382699013\n",
      "Test Done----\n",
      "number of training batch:  11\n",
      "Training----\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\ashdr\\miniconda3\\lib\\site-packages\\torch\\nn\\modules\\rnn.py:67: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.3 and num_layers=1\n",
      "  warnings.warn(\"dropout option adds dropout after all but last \"\n",
      "C:\\Users\\ashdr\\miniconda3\\lib\\site-packages\\torch\\nn\\modules\\rnn.py:67: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.3 and num_layers=1\n",
      "  warnings.warn(\"dropout option adds dropout after all but last \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iter: 100 loss：0.010411111637949944\n",
      "iter: 200 loss：0.02142507955431938\n",
      "Train Done----\n",
      "Testing----\n",
      "[[ 28.79199 -93.44741   9.9     182.19998]\n",
      " [ 29.50778 -93.36579   9.5     184.80002]\n",
      " [ 29.74811 -93.34197  10.6     172.7    ]\n",
      " [ 29.5441  -93.36136   9.5     187.6    ]\n",
      " [ 28.73532 -93.44905   9.5     183.2    ]\n",
      " [ 29.43076 -93.37343   9.6     188.29999]\n",
      " [ 29.4082  -93.3768    8.8     182.3    ]\n",
      " [ 28.88197 -93.43623   9.5     183.70001]\n",
      " [ 29.6427  -93.35006   8.8     182.6    ]\n",
      " [ 29.48673 -93.36778   9.3     181.9    ]\n",
      " [ 29.05653 -93.41964  10.      185.99998]\n",
      " [ 29.13536 -93.41263   9.      191.9    ]\n",
      " [ 28.95786 -93.42936   9.5     185.5    ]\n",
      " [ 28.97651 -93.42752   9.2     191.1    ]\n",
      " [ 28.64872 -93.45983   9.      187.     ]\n",
      " [ 29.11653 -93.41504   9.7     187.50002]\n",
      " [ 29.37763 -93.38141  10.      188.8    ]\n",
      " [ 28.87896 -93.43659   9.8     188.9    ]\n",
      " [ 29.12904 -93.4135    9.8     180.8    ]\n",
      " [ 28.50583 -93.48234   9.4     191.9    ]]\n",
      "[[ 28.861967  -93.479324    9.073092  184.43262  ]\n",
      " [ 28.879494  -93.287254    9.356871  184.42265  ]\n",
      " [ 28.934113  -93.35813     9.31573   184.46121  ]\n",
      " [ 28.86024   -93.39689     9.2350235 184.20116  ]\n",
      " [ 28.863163  -93.52482     9.070617  183.95447  ]\n",
      " [ 28.877033  -93.45861     9.039177  183.97537  ]\n",
      " [ 28.927486  -93.27431     9.475266  184.7404   ]\n",
      " [ 28.893307  -93.40368     9.242274  184.43727  ]\n",
      " [ 28.900743  -93.319885    9.310185  184.39146  ]\n",
      " [ 28.858435  -93.42537     9.087006  184.33867  ]\n",
      " [ 28.908615  -93.39529     9.228096  184.40901  ]\n",
      " [ 28.8895    -93.42609     9.210334  184.49507  ]\n",
      " [ 28.859156  -93.437454    9.200434  184.38719  ]\n",
      " [ 28.896206  -93.39812     9.220261  184.33162  ]\n",
      " [ 28.878403  -93.41588     9.257706  184.00961  ]\n",
      " [ 28.9209    -93.296776    9.263565  184.18515  ]\n",
      " [ 28.876957  -93.380424    9.243534  184.14894  ]\n",
      " [ 28.87042   -93.51594     9.153992  184.04018  ]\n",
      " [ 28.879461  -93.38266     9.207802  184.84218  ]\n",
      " [ 28.84972   -93.45345     9.149792  183.91168  ]]\n",
      "LAT:  mean_absolute_error: 0.3791474401950836, mean_squared_error: 0.20132073760032654\n",
      "LON:  mean_absolute_error: 0.05561204254627228, mean_squared_error: 0.004530740436166525\n",
      "SOG:  mean_absolute_error: 0.4463977813720703, mean_squared_error: 0.29481422901153564\n",
      "Test Done----\n",
      "number of training batch:  3\n",
      "Training----\n",
      "Train Done----\n",
      "Testing----\n",
      "[[ 2.8349710e+01 -9.3501938e+01  2.0000000e-01  2.2480000e+02]\n",
      " [ 2.8349710e+01 -9.3501892e+01  3.0000001e-01  2.2560002e+02]\n",
      " [ 2.8349710e+01 -9.3501869e+01  5.0000000e-01  3.0100000e+01]\n",
      " [ 2.8349739e+01 -9.3501953e+01  3.0000001e-01  2.3250002e+02]\n",
      " [ 2.8349760e+01 -9.3501938e+01  0.0000000e+00  1.8709999e+02]\n",
      " [ 2.8349760e+01 -9.3501892e+01  3.0000001e-01  2.0639999e+02]\n",
      " [ 2.8349710e+01 -9.3501877e+01  6.9999999e-01  2.1070000e+02]\n",
      " [ 2.8350580e+01 -9.3501846e+01  8.9999998e-01  1.9779999e+02]\n",
      " [ 2.8349739e+01 -9.3501953e+01  5.0000000e-01  1.6350000e+02]\n",
      " [ 2.8349710e+01 -9.3501930e+01  1.2000000e+00  4.7400002e+01]\n",
      " [ 2.8349739e+01 -9.3501930e+01  2.0000000e-01  2.0939999e+02]\n",
      " [ 2.8349720e+01 -9.3501938e+01  1.0000000e+00  1.1300001e+02]\n",
      " [ 2.8349751e+01 -9.3501968e+01  8.0000001e-01  2.2909999e+02]\n",
      " [ 2.8349791e+01 -9.3501968e+01  2.0000000e-01  2.0060001e+02]\n",
      " [ 2.8349760e+01 -9.3501953e+01  5.0000000e-01  2.4189999e+02]\n",
      " [ 2.8349720e+01 -9.3501953e+01  1.0000000e-01  2.0770000e+02]\n",
      " [ 2.8349751e+01 -9.3501923e+01  1.0000000e-01  2.0489999e+02]\n",
      " [ 2.8349739e+01 -9.3501900e+01  8.0000001e-01  2.5560001e+02]\n",
      " [ 2.8349720e+01 -9.3501938e+01  0.0000000e+00  2.3869998e+02]\n",
      " [ 2.8349751e+01 -9.3501999e+01  4.0000001e-01  2.3089999e+02]]\n",
      "[[ 28.385479  -93.677925    1.64177   198.22792  ]\n",
      " [ 28.364273  -93.69108     1.647357  197.79373  ]\n",
      " [ 28.167759  -94.2554      1.7988946 195.91345  ]\n",
      " [ 28.319593  -93.85875     1.588502  195.02777  ]\n",
      " [ 28.304615  -93.81609     1.8426335 197.54979  ]\n",
      " [ 28.246578  -94.08736     1.6906127 193.17996  ]\n",
      " [ 28.296995  -93.908104    1.626223  197.75851  ]\n",
      " [ 28.098982  -94.55172     1.9581803 189.56061  ]\n",
      " [ 28.339344  -93.75071     1.5795534 198.57886  ]\n",
      " [ 27.956432  -95.32357     1.3953744 175.9771   ]\n",
      " [ 28.29382   -93.98556     1.4546213 195.32379  ]\n",
      " [ 28.355698  -93.81468     1.4543    196.49156  ]\n",
      " [ 28.295805  -93.96076     1.524896  192.88036  ]\n",
      " [ 28.299967  -93.99874     1.4700145 196.91553  ]\n",
      " [ 28.38425   -93.58206     1.7334279 200.82593  ]\n",
      " [ 28.434374  -93.49506     1.6662673 199.66875  ]\n",
      " [ 28.36558   -93.72488     1.6334771 197.23715  ]\n",
      " [ 28.23392   -94.24468     1.9658293 192.54608  ]\n",
      " [ 28.402542  -93.55313     1.6207778 199.33081  ]\n",
      " [ 28.33139   -93.87047     1.6208674 195.30743  ]]\n",
      "LAT:  mean_absolute_error: 0.06461617350578308, mean_squared_error: 0.0106265963986516\n",
      "LON:  mean_absolute_error: 0.3905166983604431, mean_squared_error: 0.2716245651245117\n",
      "SOG:  mean_absolute_error: 1.204382300376892, mean_squared_error: 1.658235788345337\n",
      "Test Done----\n",
      "number of training batch:  4\n",
      "Training----\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\ashdr\\miniconda3\\lib\\site-packages\\torch\\nn\\modules\\rnn.py:67: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.3 and num_layers=1\n",
      "  warnings.warn(\"dropout option adds dropout after all but last \"\n",
      "C:\\Users\\ashdr\\miniconda3\\lib\\site-packages\\torch\\nn\\modules\\rnn.py:67: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.3 and num_layers=1\n",
      "  warnings.warn(\"dropout option adds dropout after all but last \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Done----\n",
      "Testing----\n",
      "[[ 38.0125   -90.07157    5.4      277.1     ]\n",
      " [ 38.19312  -90.3355     5.3      325.      ]\n",
      " [ 38.08962  -90.21008    5.2      303.      ]\n",
      " [ 38.09575  -90.22211    5.1      305.      ]\n",
      " [ 38.05765  -90.12231    4.4      342.      ]\n",
      " [ 38.24485  -90.36578    5.1      355.      ]\n",
      " [ 38.06571  -90.13548    5.       273.      ]\n",
      " [ 38.004158 -90.04808    5.6      299.2     ]\n",
      " [ 38.01003  -90.06131    5.4      296.      ]\n",
      " [ 38.08082  -90.19596    5.6      309.      ]\n",
      " [ 38.26162  -90.36938    4.9      347.      ]\n",
      " [ 38.09301  -90.21791    5.1      310.      ]\n",
      " [ 38.2048   -90.34599    5.       329.8     ]\n",
      " [ 38.22067  -90.35693    5.1      333.      ]\n",
      " [ 38.04667  -90.12095    5.7      352.      ]\n",
      " [ 38.01071  -90.06333    5.4      292.      ]\n",
      " [ 38.12315  -90.25069    5.1      334.      ]\n",
      " [ 38.00505  -90.05002    5.6      301.      ]\n",
      " [ 38.17981  -90.31918    5.1      291.      ]\n",
      " [ 38.19675  -90.33886    5.1      320.3     ]]\n",
      "[[ 37.4433    -90.681175    6.1910667 280.01855  ]\n",
      " [ 38.07055   -89.84824     5.712807  298.87708  ]\n",
      " [ 37.7212    -90.21632     6.129922  287.46396  ]\n",
      " [ 37.720093  -90.41329     6.0754027 287.3228   ]\n",
      " [ 33.481236  -95.28953     7.9299207 173.01842  ]\n",
      " [ 38.291107  -89.73129     5.5857706 304.62747  ]\n",
      " [ 37.4244    -90.663895    6.1135354 279.4918   ]\n",
      " [ 37.53978   -90.62794     6.114538  283.58047  ]\n",
      " [ 37.49752   -90.56709     6.161544  281.46255  ]\n",
      " [ 37.820736  -90.154495    6.069102  290.035    ]\n",
      " [ 38.435146  -89.49785     5.39961   309.76352  ]\n",
      " [ 37.567154  -90.55394     6.0336866 283.74097  ]\n",
      " [ 38.030445  -89.93385     5.79865   297.83755  ]\n",
      " [ 38.216778  -89.69789     5.5271688 303.913    ]\n",
      " [ 38.486633  -89.322205    5.651043  309.7713   ]\n",
      " [ 37.529423  -90.5052      6.2172966 281.236    ]\n",
      " [ 38.2825    -89.62071     5.670919  303.9416   ]\n",
      " [ 37.59555   -90.49433     6.19655   282.8363   ]\n",
      " [ 37.26341   -90.8185      6.3721366 275.7496   ]\n",
      " [ 38.046764  -89.895       6.0880656 296.31305  ]]\n",
      "LAT:  mean_absolute_error: 0.4581046998500824, mean_squared_error: 0.6782755851745605\n",
      "LON:  mean_absolute_error: 0.5878425240516663, mean_squared_error: 0.9002848863601685\n",
      "SOG:  mean_absolute_error: 0.776592493057251, mean_squared_error: 0.886573851108551\n",
      "Test Done----\n",
      "number of training batch:  5\n",
      "Training----\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\ashdr\\miniconda3\\lib\\site-packages\\torch\\nn\\modules\\rnn.py:67: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.3 and num_layers=1\n",
      "  warnings.warn(\"dropout option adds dropout after all but last \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iter: 100 loss：1.043613314628601\n",
      "Train Done----\n",
      "Testing----\n",
      "[[ 3.836152e+01 -9.035643e+01  4.700000e+00  1.700000e+01]\n",
      " [ 3.858058e+01 -9.021870e+01  3.000000e-01  3.550000e+02]\n",
      " [ 3.855533e+01 -9.024247e+01  4.900000e+00  3.600000e+01]\n",
      " [ 3.842932e+01 -9.029118e+01  4.800000e+00  2.900000e+01]\n",
      " [ 3.842804e+01 -9.029224e+01  4.800000e+00  3.400000e+01]\n",
      " [ 3.836627e+01 -9.035453e+01  5.100000e+00  8.000000e+00]\n",
      " [ 3.860028e+01 -9.019428e+01  1.000000e-01  3.180000e+02]\n",
      " [ 3.853335e+01 -9.025754e+01  4.900000e+00  2.500000e+01]\n",
      " [ 3.841481e+01 -9.031219e+01  5.600000e+00  5.600000e+01]\n",
      " [ 3.836558e+01 -9.035469e+01  5.000000e+00  1.400000e+01]\n",
      " [ 3.854687e+01 -9.024863e+01  4.800000e+00  2.050000e+01]\n",
      " [ 3.856772e+01 -9.023461e+01  1.000000e-01  1.300000e+01]\n",
      " [ 3.843812e+01 -9.028667e+01  4.600000e+00  1.800000e+01]\n",
      " [ 3.857423e+01 -9.022650e+01  2.200000e+00  4.700000e+01]\n",
      " [ 3.841097e+01 -9.031902e+01  4.100000e+00  3.400000e+01]\n",
      " [ 3.839043e+01 -9.034090e+01  4.500000e+00  4.700000e+01]\n",
      " [ 3.849073e+01 -9.027561e+01  4.800000e+00  9.000000e+00]\n",
      " [ 3.843362e+01 -9.028877e+01  4.800000e+00  2.300000e+01]\n",
      " [ 3.856080e+01 -9.023870e+01  4.700000e+00  3.000000e+01]\n",
      " [ 3.852900e+01 -9.026033e+01  4.900000e+00  2.730000e+01]]\n",
      "[[ 38.37485   -90.46814     3.7685618  27.180534 ]\n",
      " [ 38.23414   -90.65262     3.3868787  48.143257 ]\n",
      " [ 38.278324  -90.64187     3.4009442  43.391613 ]\n",
      " [ 38.277985  -90.497826    3.6283197  43.409428 ]\n",
      " [ 38.334476  -90.50337     3.5634744  45.442177 ]\n",
      " [ 38.24166   -90.63578     3.45693    27.09671  ]\n",
      " [ 38.295902  -90.60089     3.3562155  23.389774 ]\n",
      " [ 38.266373  -90.66583     3.4116828  33.818253 ]\n",
      " [ 38.13178   -90.74246     3.3595448  59.23791  ]\n",
      " [ 38.251865  -90.62979     3.3610196  27.488728 ]\n",
      " [ 38.201195  -90.61855     3.4592662  37.55254  ]\n",
      " [ 38.651722  -89.99698     3.953447  216.34564  ]\n",
      " [ 38.344913  -90.51461     3.4335825  29.394468 ]\n",
      " [ 38.25868   -90.561264    3.4580672  47.89618  ]\n",
      " [ 38.205303  -90.68031     3.3718996  45.522316 ]\n",
      " [ 38.206314  -90.68578     3.51627    46.513832 ]\n",
      " [ 37.004913  -91.965126    1.5716145  67.03457  ]\n",
      " [ 38.227253  -90.666565    3.3387256  33.362442 ]\n",
      " [ 38.284584  -90.55409     3.4541514  38.100204 ]\n",
      " [ 38.255142  -90.6024      3.2354028  38.002346 ]]\n",
      "LAT:  mean_absolute_error: 0.2088061422109604, mean_squared_error: 0.09050094336271286\n",
      "LON:  mean_absolute_error: 0.31573957204818726, mean_squared_error: 0.15174956619739532\n",
      "SOG:  mean_absolute_error: 1.4758999347686768, mean_squared_error: 2.693293571472168\n",
      "Test Done----\n",
      "number of training batch:  34\n",
      "Training----\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\ashdr\\miniconda3\\lib\\site-packages\\torch\\nn\\modules\\rnn.py:67: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.3 and num_layers=1\n",
      "  warnings.warn(\"dropout option adds dropout after all but last \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iter: 100 loss：1.010598063468933\n",
      "iter: 200 loss：0.21896010637283325\n",
      "iter: 300 loss：1.899187445640564\n",
      "iter: 400 loss：0.9307044744491577\n",
      "iter: 500 loss：0.16603393852710724\n",
      "iter: 600 loss：0.07573643326759338\n",
      "Train Done----\n",
      "Testing----\n",
      "[[ 3.9966709e+01 -8.0743759e+01  4.3000002e+00  3.3000000e+01]\n",
      " [ 4.0264858e+01 -7.9898628e+01  3.0999999e+00  2.2089999e+02]\n",
      " [ 3.9996319e+01 -8.0738640e+01  4.1999998e+00  3.5300000e+02]\n",
      " [ 3.9993790e+01 -7.9948402e+01  2.8000000e+00  2.7879999e+02]\n",
      " [ 3.9742008e+01 -8.0851517e+01  4.4000001e+00  3.0400000e+02]\n",
      " [ 3.9987919e+01 -8.0737633e+01  4.3000002e+00  3.5500000e+02]\n",
      " [ 4.0233871e+01 -8.0657860e+01  4.4000001e+00  1.8000000e+01]\n",
      " [ 4.0134930e+01 -8.0704697e+01  1.1000000e+00  8.0000000e+00]\n",
      " [ 4.0176640e+01 -7.9850151e+01  5.0000000e+00  1.5000000e+00]\n",
      " [ 4.0253151e+01 -8.0638672e+01  4.3000002e+00  5.3000000e+01]\n",
      " [ 4.0240299e+01 -7.9957253e+01  4.8000002e+00  2.5080002e+02]\n",
      " [ 4.0044540e+01 -7.9889290e+01  3.5999999e+00  1.4220000e+02]\n",
      " [ 3.9602859e+01 -8.0947601e+01  4.0000000e+00  6.0000000e+01]\n",
      " [ 4.0243961e+01 -7.9927223e+01  6.0999999e+00  6.9800003e+01]\n",
      " [ 3.9649200e+01 -8.0868027e+01  1.0000000e+00  1.0000000e+01]\n",
      " [ 4.0076328e+01 -7.9862617e+01  3.6999998e+00  2.5910001e+02]\n",
      " [ 4.0165401e+01 -7.9889702e+01  4.1999998e+00  2.7279999e+02]\n",
      " [ 3.9971722e+01 -8.0739922e+01  4.4000001e+00  1.5000000e+01]\n",
      " [ 4.0316109e+01 -7.9891731e+01  1.0000000e-01  3.6000000e+02]\n",
      " [ 4.0255630e+01 -8.0633904e+01  4.6999998e+00  5.8999996e+01]]\n",
      "[[ 40.403664  -80.009964    4.425386   67.69626  ]\n",
      " [ 40.810093  -79.25471     4.5430284 212.25313  ]\n",
      " [ 41.150963  -78.716835    4.3021345 320.8056   ]\n",
      " [ 40.88892   -79.06977     4.2086964 248.4083   ]\n",
      " [ 40.968414  -79.03536     4.4212723 276.28027  ]\n",
      " [ 40.45377   -80.13456     3.9212806  30.17602  ]\n",
      " [ 40.38851   -80.190414    4.295356   38.044636 ]\n",
      " [ 40.201958  -80.38694     3.5715582  48.191196 ]\n",
      " [ 40.445652  -80.05854     4.270334   25.985535 ]\n",
      " [ 40.37197   -80.05644     4.355844   67.252266 ]\n",
      " [ 40.840916  -79.14019     4.6084094 242.06625  ]\n",
      " [ 40.684166  -79.4141      4.472378  183.79263  ]\n",
      " [ 40.326214  -80.15085     4.4494653  71.01764  ]\n",
      " [ 40.67664   -79.5342      4.788907   79.91302  ]\n",
      " [ 40.961655  -79.04855     4.0005565 321.85684  ]\n",
      " [ 40.80426   -79.20958     4.517693  240.93068  ]\n",
      " [ 40.994953  -78.90884     4.5063844 256.36438  ]\n",
      " [ 40.409805  -80.017914    4.360969   43.98828  ]\n",
      " [ 40.525677  -79.76273     3.6225393  97.80444  ]\n",
      " [ 40.44233   -79.95583     4.435566   71.67371  ]]\n",
      "LAT:  mean_absolute_error: 0.6091085076332092, mean_squared_error: 0.46585291624069214\n",
      "LON:  mean_absolute_error: 0.7936597466468811, mean_squared_error: 0.8763650059700012\n",
      "SOG:  mean_absolute_error: 0.7007513046264648, mean_squared_error: 1.0637139081954956\n",
      "Test Done----\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\ashdr\\miniconda3\\lib\\site-packages\\torch\\nn\\modules\\rnn.py:67: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.3 and num_layers=1\n",
      "  warnings.warn(\"dropout option adds dropout after all but last \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "number of training batch:  13\n",
      "Training----\n",
      "iter: 100 loss：0.15218301117420197\n",
      "iter: 200 loss：0.13346043229103088\n",
      "Train Done----\n",
      "Testing----\n",
      "[[ 42.60375   -82.52842     9.3       229.3      ]\n",
      " [ 43.14806   -82.40963    12.3       178.4      ]\n",
      " [ 42.54551   -82.60404     7.8       245.19998  ]\n",
      " [ 42.77927   -82.47045    10.3       168.6      ]\n",
      " [ 43.56327   -82.42739    12.7       175.2      ]\n",
      " [ 42.590633  -82.54867    10.2       220.1      ]\n",
      " [ 42.36541   -82.90006    12.4       238.4      ]\n",
      " [ 42.65089   -82.51123    10.2       178.3      ]\n",
      " [ 42.90668   -82.46353     9.9       198.9      ]\n",
      " [ 43.42278   -82.41135    12.8       180.60002  ]\n",
      " [ 42.53134   -82.64387     7.4999995 244.40001  ]\n",
      " [ 42.73071   -82.48359    10.        191.6      ]\n",
      " [ 43.22289   -82.4115     12.5       178.3      ]\n",
      " [ 42.7484    -82.47392     9.3       205.7      ]\n",
      " [ 42.887974  -82.47301    10.        187.       ]\n",
      " [ 42.12261   -83.12507    11.        186.1      ]\n",
      " [ 42.28986   -83.09624     9.        206.20001  ]\n",
      " [ 42.53775   -82.62589     7.6999993 243.99998  ]\n",
      " [ 42.26848   -83.10862     8.7       202.1      ]\n",
      " [ 42.26047   -83.11314     9.1       203.6      ]]\n",
      "[[ 43.163544  -82.249825   11.321783  197.06216  ]\n",
      " [ 42.832714  -82.46141    11.7281475 186.45912  ]\n",
      " [ 43.205795  -82.23656    11.111435  204.15204  ]\n",
      " [ 42.669968  -82.57612    11.76048   184.03839  ]\n",
      " [ 42.801296  -82.565895   11.796201  186.376    ]\n",
      " [ 43.019123  -82.31154    11.366832  198.00984  ]\n",
      " [ 43.07302   -82.248055   11.352004  198.06175  ]\n",
      " [ 42.736748  -82.62484    11.496761  187.4248   ]\n",
      " [ 42.885925  -82.49776    11.294542  191.88828  ]\n",
      " [ 42.719402  -82.71761    11.710269  186.51163  ]\n",
      " [ 43.234566  -82.064125   11.118466  203.87172  ]\n",
      " [ 42.855465  -82.441414   11.425721  191.37994  ]\n",
      " [ 42.755413  -82.482025   11.82212   184.76125  ]\n",
      " [ 42.92347   -82.440254   11.348823  192.7865   ]\n",
      " [ 42.99486   -82.34158    11.405817  193.54239  ]\n",
      " [ 42.73638   -82.55826    11.657081  187.63899  ]\n",
      " [ 42.97094   -82.33875    11.27355   194.13246  ]\n",
      " [ 43.23985   -82.11156    11.067499  204.0334   ]\n",
      " [ 42.801212  -82.663216   11.247088  193.25354  ]\n",
      " [ 42.927113  -82.488045   11.312589  193.94246  ]]\n",
      "LAT:  mean_absolute_error: 0.5543248653411865, mean_squared_error: 0.39323005080223083\n",
      "LON:  mean_absolute_error: 0.28235912322998047, mean_squared_error: 0.14361780881881714\n",
      "SOG:  mean_absolute_error: 1.4670162200927734, mean_squared_error: 3.0506160259246826\n",
      "Test Done----\n",
      "number of training batch:  4\n",
      "Training----\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\ashdr\\miniconda3\\lib\\site-packages\\torch\\nn\\modules\\rnn.py:67: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.3 and num_layers=1\n",
      "  warnings.warn(\"dropout option adds dropout after all but last \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Done----\n",
      "Testing----\n",
      "[[ 41.695248 -82.17421   12.5      116.      ]\n",
      " [ 41.67997  -82.13211   12.6      115.899994]\n",
      " [ 41.60519  -81.92786   12.5      116.80001 ]\n",
      " [ 41.78225  -82.39361   12.3      118.4     ]\n",
      " [ 41.53803  -81.75935    9.5      124.2     ]\n",
      " [ 41.72156  -82.24176   12.5      117.9     ]\n",
      " [ 41.67311  -82.11312   12.6      115.6     ]\n",
      " [ 41.75     -82.31341   12.4      117.99999 ]\n",
      " [ 41.53202  -81.75118    8.2      136.4     ]\n",
      " [ 41.55105  -81.78883   11.       120.3     ]\n",
      " [ 41.79934  -82.43619   12.       118.4     ]\n",
      " [ 41.812416 -82.4682    11.9      120.2     ]\n",
      " [ 41.67466  -82.11741   12.6      115.700005]\n",
      " [ 41.83833  -82.5296    11.6      118.6     ]\n",
      " [ 41.59868  -81.9106    12.5      116.7     ]\n",
      " [ 41.84816  -82.55101   11.3      122.7     ]\n",
      " [ 41.7481   -82.30866   12.4      118.19999 ]\n",
      " [ 41.79004  -82.41298   12.1      118.19999 ]\n",
      " [ 41.53965  -81.76288    9.6      120.3     ]\n",
      " [ 41.66779  -82.09842   12.6      115.700005]]\n",
      "[[ 41.861656  -81.76531    12.719256  119.40412  ]\n",
      " [ 41.72794   -81.86999    12.777212  119.441864 ]\n",
      " [ 41.82671   -81.77183    12.80846   119.40955  ]\n",
      " [ 41.746338  -81.889885   12.913672  119.931335 ]\n",
      " [ 41.765717  -81.84615    12.49178   120.14793  ]\n",
      " [ 41.70921   -82.14773    12.769161  118.745705 ]\n",
      " [ 41.730293  -81.91568    12.729456  119.56634  ]\n",
      " [ 41.94932   -81.767975   12.676006  119.79272  ]\n",
      " [ 41.863785  -81.82203    12.8005705 119.97568  ]\n",
      " [ 41.79807   -81.86981    12.792644  118.303665 ]\n",
      " [ 41.9088    -81.7568     12.908198  119.743164 ]\n",
      " [ 41.845657  -81.84857    12.879817  118.86045  ]\n",
      " [ 41.740368  -81.8558     12.929935  119.061615 ]\n",
      " [ 41.86539   -81.78649    12.737551  120.15684  ]\n",
      " [ 41.890793  -81.82625    12.894396  119.16192  ]\n",
      " [ 41.86067   -81.78048    12.943757  120.33204  ]\n",
      " [ 41.806686  -81.85288    12.917465  118.80078  ]\n",
      " [ 41.90048   -81.77977    13.07996   119.45872  ]\n",
      " [ 41.725304  -81.932724   12.95379   118.38305  ]\n",
      " [ 41.97436   -81.66546    12.778174  120.366585 ]]\n",
      "LAT:  mean_absolute_error: 0.12848705053329468, mean_squared_error: 0.026008807122707367\n",
      "LON:  mean_absolute_error: 0.35464033484458923, mean_squared_error: 0.17523950338363647\n",
      "SOG:  mean_absolute_error: 0.8760824203491211, mean_squared_error: 1.6935224533081055\n",
      "Test Done----\n",
      "number of training batch:  4\n",
      "Training----\n",
      "Train Done----\n",
      "Testing----\n",
      "[[ 4.41165199e+01 -1.24282463e+02  1.01000004e+01  3.35200012e+02]\n",
      " [ 4.42551613e+01 -1.24377037e+02  9.69999981e+00  3.47299988e+02]\n",
      " [ 4.41904602e+01 -1.24338875e+02  6.30000019e+00  3.33700012e+02]\n",
      " [ 4.42274590e+01 -1.24365128e+02  9.60000038e+00  3.33299988e+02]\n",
      " [ 4.40915642e+01 -1.24271149e+02  1.01999998e+01  3.48899994e+02]\n",
      " [ 4.38061409e+01 -1.24266953e+02  1.10000000e+01  3.57799988e+02]\n",
      " [ 4.41371918e+01 -1.24301285e+02  1.01000004e+01  3.26500000e+02]\n",
      " [ 4.40767593e+01 -1.24268059e+02  1.03000002e+01  3.50700012e+02]\n",
      " [ 4.42301903e+01 -1.24366928e+02  9.80000019e+00  3.35399994e+02]\n",
      " [ 4.41433716e+01 -1.24305832e+02  1.01000004e+01  3.33299988e+02]\n",
      " [ 4.37483406e+01 -1.24268570e+02  1.03999996e+01  3.00000012e-01]\n",
      " [ 4.44438705e+01 -1.24452843e+02  9.19999981e+00  3.43299988e+02]\n",
      " [ 4.42958984e+01 -1.24393440e+02  9.39999962e+00  3.45700012e+02]\n",
      " [ 4.37488098e+01 -1.24136292e+02  5.40000010e+00  2.73600006e+02]\n",
      " [ 4.41112404e+01 -1.24278580e+02  1.00000000e+01  3.33200012e+02]\n",
      " [ 4.40707512e+01 -1.24266899e+02  1.03999996e+01  3.52600006e+02]\n",
      " [ 4.37458992e+01 -1.24131348e+02  6.40000010e+00  3.46700012e+02]\n",
      " [ 4.43355331e+01 -1.24409378e+02  9.30000019e+00  3.41899994e+02]\n",
      " [ 4.43258896e+01 -1.24405312e+02  9.50000000e+00  3.43600006e+02]\n",
      " [ 4.40944214e+01 -1.24271904e+02  1.01999998e+01  3.47799988e+02]]\n",
      "[[  45.092426  -124.09759      9.757359   308.9916   ]\n",
      " [  45.1128    -124.04922      9.321421   314.36584  ]\n",
      " [  45.22321   -124.06883      9.359606   308.75977  ]\n",
      " [  45.16428   -124.05167      9.532594   303.62332  ]\n",
      " [  45.010498  -124.0106       9.392371   313.90707  ]\n",
      " [  43.69477   -123.9641      11.002626    86.728195 ]\n",
      " [  45.307465  -124.037865     9.608213   301.03857  ]\n",
      " [  45.166256  -124.00305      9.203782   323.12485  ]\n",
      " [  44.953506  -124.0668       9.350436   300.12418  ]\n",
      " [  45.138653  -124.05584      9.460649   306.04666  ]\n",
      " [  45.51159   -123.648964    11.0822315  181.6902   ]\n",
      " [  44.985394  -124.05706      9.449691   316.31314  ]\n",
      " [  45.516506  -124.14152      9.54337    315.0037   ]\n",
      " [  44.973133  -124.16882      9.483455   300.36594  ]\n",
      " [  45.161625  -124.02218      9.201807   314.39594  ]\n",
      " [  45.07795   -124.022064     9.589698   313.1622   ]\n",
      " [  45.062717  -124.04238      9.273388   312.46967  ]\n",
      " [  45.22724   -124.02562      9.70609    313.4673   ]\n",
      " [  45.34908   -124.101265     9.457683   311.61554  ]\n",
      " [  45.14462   -124.015465     9.647417   314.52884  ]]\n",
      "LAT:  mean_absolute_error: 0.9460933208465576, mean_squared_error: 1.0091736316680908\n",
      "LON:  mean_absolute_error: 0.26905691623687744, mean_squared_error: 0.08350072801113129\n",
      "SOG:  mean_absolute_error: 0.7967230677604675, mean_squared_error: 1.6002848148345947\n",
      "Test Done----\n",
      "number of training batch:  10\n",
      "Training----\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\ashdr\\miniconda3\\lib\\site-packages\\torch\\nn\\modules\\rnn.py:67: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.3 and num_layers=1\n",
      "  warnings.warn(\"dropout option adds dropout after all but last \"\n",
      "C:\\Users\\ashdr\\miniconda3\\lib\\site-packages\\torch\\nn\\modules\\rnn.py:67: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.3 and num_layers=1\n",
      "  warnings.warn(\"dropout option adds dropout after all but last \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iter: 100 loss：0.10935482382774353\n",
      "iter: 200 loss：0.13178759813308716\n",
      "Train Done----\n",
      "Testing----\n",
      "[[ 28.58976  -89.28275   11.9      154.7     ]\n",
      " [ 28.73512  -89.37354   11.7      151.2     ]\n",
      " [ 27.97314  -88.94692   11.7      156.8     ]\n",
      " [ 29.92308  -89.94      12.4       90.1     ]\n",
      " [ 28.75403  -89.38562   11.6      151.2     ]\n",
      " [ 29.52364  -89.72772   14.599999 113.00001 ]\n",
      " [ 29.45203  -89.60549   14.2      118.9     ]\n",
      " [ 28.56129  -89.26731   11.8      154.2     ]\n",
      " [ 28.15962  -89.04172   11.9      154.4     ]\n",
      " [ 28.02899  -88.97406   11.9      156.6     ]\n",
      " [ 27.813879 -88.85687   11.4      151.      ]\n",
      " [ 29.49386  -89.69915   14.4      150.2     ]\n",
      " [ 29.51902  -89.71868   13.8      133.6     ]\n",
      " [ 29.88106  -89.90248   13.299999 174.6     ]\n",
      " [ 29.50146  -89.70415   14.1      149.7     ]\n",
      " [ 28.760202 -89.38958   11.6      150.9     ]\n",
      " [ 29.71844  -89.98578   14.1      154.4     ]\n",
      " [ 29.35767  -89.46261   12.7       35.1     ]\n",
      " [ 28.21255  -89.06989   11.9      154.4     ]\n",
      " [ 29.616568 -89.89854   14.3      125.3     ]]\n",
      "[[ 28.751644  -89.20589    12.317744  147.80885  ]\n",
      " [ 28.730476  -89.3043     12.355983  145.2315   ]\n",
      " [ 28.623411  -89.452675   12.1730385 149.15123  ]\n",
      " [ 29.172451  -90.39637    12.685189   98.52734  ]\n",
      " [ 28.721443  -89.31578    12.253302  144.60623  ]\n",
      " [ 29.046415  -90.02618    12.542162  117.52198  ]\n",
      " [ 29.053406  -90.08922    12.715334  113.75094  ]\n",
      " [ 28.793077  -89.104805   12.51502   148.37328  ]\n",
      " [ 28.624018  -89.2888     12.260277  147.74643  ]\n",
      " [ 28.712803  -89.13723    12.277139  148.0224   ]\n",
      " [ 28.693436  -89.353516   12.433042  148.16522  ]\n",
      " [ 28.884138  -89.356415   12.739472  143.16805  ]\n",
      " [ 29.085602  -89.87802    12.8780365 119.012184 ]\n",
      " [ 28.76078   -88.8584     12.558678  155.88396  ]\n",
      " [ 28.975145  -89.18598    12.627625  139.53186  ]\n",
      " [ 28.808678  -89.176125   12.383056  145.06908  ]\n",
      " [ 28.959927  -89.213455   12.583075  139.22649  ]\n",
      " [ 29.385323  -91.55053    12.843468   70.549736 ]\n",
      " [ 28.637768  -89.29955    12.317553  146.80302  ]\n",
      " [ 29.106737  -89.55968    12.634472  125.999794 ]]\n",
      "LAT:  mean_absolute_error: 0.4600381553173065, mean_squared_error: 0.28776782751083374\n",
      "LON:  mean_absolute_error: 0.32540690898895264, mean_squared_error: 0.23306098580360413\n",
      "SOG:  mean_absolute_error: 0.9172728657722473, mean_squared_error: 1.1539467573165894\n",
      "Test Done----\n",
      "number of training batch:  5\n",
      "Training----\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\ashdr\\miniconda3\\lib\\site-packages\\torch\\nn\\modules\\rnn.py:67: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.3 and num_layers=1\n",
      "  warnings.warn(\"dropout option adds dropout after all but last \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iter: 100 loss：0.020805612206459045\n",
      "Train Done----\n",
      "Testing----\n",
      "[[ 24.32393  -81.37655   14.799999  75.      ]\n",
      " [ 24.23315  -81.74052   15.8       74.      ]\n",
      " [ 24.30447  -81.45367   15.3       74.      ]\n",
      " [ 24.4306   -80.96743   15.399999  75.      ]\n",
      " [ 24.45997  -80.84758   15.399999  74.      ]\n",
      " [ 24.54978  -80.51525   17.3       67.      ]\n",
      " [ 24.3639   -81.22312   14.599999  72.      ]\n",
      " [ 24.427    -80.98248   15.399999  74.      ]\n",
      " [ 24.25998  -81.63373   14.999999  74.      ]\n",
      " [ 24.37237  -81.19433   14.799999  72.      ]\n",
      " [ 24.31513  -81.41143   15.1       74.      ]\n",
      " [ 24.6063   -80.35565   18.2       42.      ]\n",
      " [ 24.43813  -80.93657   15.399999  74.      ]\n",
      " [ 24.24273  -81.7022    15.5       74.      ]\n",
      " [ 24.33803  -81.31792   14.599999  75.      ]\n",
      " [ 24.39308  -81.11963   14.999999  74.      ]\n",
      " [ 24.37885  -81.17208   14.799999  72.      ]\n",
      " [ 24.47903  -80.77018   15.1       74.      ]\n",
      " [ 24.29003  -81.51212   15.199999  75.      ]\n",
      " [ 24.48563  -80.74345   15.5       74.      ]]\n",
      "[[ 24.28161  -81.076294  15.009108  70.408165]\n",
      " [ 24.34342  -81.02844   15.083924  71.52675 ]\n",
      " [ 24.37389  -80.93416   15.234821  71.045616]\n",
      " [ 24.320042 -80.9459    15.158498  70.34345 ]\n",
      " [ 24.39192  -80.90902   15.268681  71.0386  ]\n",
      " [ 24.33229  -81.033806  15.249497  70.75616 ]\n",
      " [ 24.296087 -81.12268   15.106632  70.31282 ]\n",
      " [ 24.349733 -80.9225    15.179367  70.55252 ]\n",
      " [ 24.35509  -80.93595   15.377235  71.25894 ]\n",
      " [ 24.347418 -80.68937   14.945145  70.18757 ]\n",
      " [ 24.318552 -81.00263   15.243251  70.720345]\n",
      " [ 24.29684  -81.21781   14.96385   70.54628 ]\n",
      " [ 24.359098 -80.898544  15.39513   70.95126 ]\n",
      " [ 24.404335 -81.036415  15.488452  72.013084]\n",
      " [ 24.399517 -81.01424   15.302345  72.02888 ]\n",
      " [ 24.331602 -81.11785   15.270104  70.64708 ]\n",
      " [ 24.40387  -80.73875   15.300504  71.18826 ]\n",
      " [ 24.358255 -81.136765  15.529278  71.43027 ]\n",
      " [ 24.348383 -80.72724   15.304781  69.86962 ]\n",
      " [ 24.386057 -80.4876    15.295323  70.46966 ]]\n",
      "LAT:  mean_absolute_error: 0.1011631116271019, mean_squared_error: 0.015370017848908901\n",
      "LON:  mean_absolute_error: 0.32055744528770447, mean_squared_error: 0.16443154215812683\n",
      "SOG:  mean_absolute_error: 0.6572269797325134, mean_squared_error: 0.9910890460014343\n",
      "Test Done----\n",
      "number of training batch:  13\n",
      "Training----\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\ashdr\\miniconda3\\lib\\site-packages\\torch\\nn\\modules\\rnn.py:67: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.3 and num_layers=1\n",
      "  warnings.warn(\"dropout option adds dropout after all but last \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iter: 100 loss：0.024181270971894264\n",
      "iter: 200 loss：0.030167508870363235\n",
      "Train Done----\n",
      "Testing----\n",
      "[[ 25.48183  -79.90905   18.3        6.      ]\n",
      " [ 27.888678 -79.66673   18.5        6.      ]\n",
      " [ 27.81128  -79.67533   18.7        5.      ]\n",
      " [ 26.05873  -79.82628   17.9        9.      ]\n",
      " [ 25.95912  -79.84472   17.9        9.      ]\n",
      " [ 26.10793  -79.81952   17.9        1.      ]\n",
      " [ 26.63588  -79.78262   18.9        8.      ]\n",
      " [ 26.494051 -79.80232   18.7        5.      ]\n",
      " [ 26.64775  -79.78065   19.         8.      ]\n",
      " [ 25.70713  -79.8843    18.1        5.      ]\n",
      " [ 25.643778 -79.89028   17.9        5.      ]\n",
      " [ 28.01728  -79.65175   18.4        5.      ]\n",
      " [ 26.208551 -79.81653   18.2        1.      ]\n",
      " [ 27.63875  -79.69105   19.         4.      ]\n",
      " [ 26.49948  -79.80177   18.6        5.      ]\n",
      " [ 26.37517  -79.81007   18.4        2.      ]\n",
      " [ 27.4115   -79.71318   19.4        5.      ]\n",
      " [ 27.82272  -79.6741    18.7        5.      ]\n",
      " [ 27.9383   -79.66095   18.5        6.      ]\n",
      " [ 26.63013  -79.78355   18.9        8.      ]]\n",
      "[[ 25.914272   -79.84722     18.006569     2.9964359 ]\n",
      " [ 26.332495   -79.374535    18.765924     3.9994142 ]\n",
      " [ 26.330967   -79.380875    18.816689     3.5144904 ]\n",
      " [ 25.948006   -79.89431     18.17617      4.818666  ]\n",
      " [ 25.979202   -79.75368     18.236658     4.3897295 ]\n",
      " [ 26.031052   -79.63746     18.310709    -1.7553347 ]\n",
      " [ 26.193546   -79.709946    18.645641     5.510165  ]\n",
      " [ 26.096575   -79.539505    18.418198     2.872637  ]\n",
      " [ 26.151138   -79.47782     18.48035      5.524199  ]\n",
      " [ 26.050493   -79.60134     18.457397     2.1649241 ]\n",
      " [ 25.960098   -79.75679     18.255886     2.3349044 ]\n",
      " [ 26.35762    -79.29171     18.848465     4.3089333 ]\n",
      " [ 26.076883   -79.71075     18.464157    -2.176447  ]\n",
      " [ 26.387512   -79.2357      18.870451     3.6796694 ]\n",
      " [ 26.17964    -79.5818      18.517904     2.0117633 ]\n",
      " [ 26.171562   -79.499054    18.642303     0.16504169]\n",
      " [ 26.34223    -79.269516    18.833544     3.004166  ]\n",
      " [ 26.352652   -79.32456     18.839146     3.5156798 ]\n",
      " [ 26.391376   -79.27605     19.033466     3.406946  ]\n",
      " [ 26.190931   -79.45798     18.686926     6.0636973 ]]\n",
      "LAT:  mean_absolute_error: 0.7073731422424316, mean_squared_error: 0.7668839693069458\n",
      "LON:  mean_absolute_error: 0.21612748503684998, mean_squared_error: 0.06646963208913803\n",
      "SOG:  mean_absolute_error: 0.3562341332435608, mean_squared_error: 0.21068596839904785\n",
      "Test Done----\n",
      "number of training batch:  4\n",
      "Training----\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\ashdr\\miniconda3\\lib\\site-packages\\torch\\nn\\modules\\rnn.py:67: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.3 and num_layers=1\n",
      "  warnings.warn(\"dropout option adds dropout after all but last \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Done----\n",
      "Testing----\n",
      "[[ 28.73538  -79.6593     2.8      346.      ]\n",
      " [ 28.78872  -79.67562    2.7      345.      ]\n",
      " [ 28.707668 -79.65263    2.6      349.      ]\n",
      " [ 28.67587  -79.64765    2.8      354.      ]\n",
      " [ 28.79198  -79.67647    3.       347.      ]\n",
      " [ 28.5753   -79.65768    3.2        1.      ]\n",
      " [ 28.73463  -79.6591     2.8      346.      ]\n",
      " [ 28.77733  -79.67152    3.       345.      ]\n",
      " [ 28.76277  -79.66698    2.9      344.      ]\n",
      " [ 28.74108  -79.66082    2.6      346.      ]\n",
      " [ 28.67487  -79.64755    2.7      355.      ]\n",
      " [ 28.75802  -79.6656     3.       347.      ]\n",
      " [ 28.74565  -79.66203    2.9      346.      ]\n",
      " [ 28.73018  -79.65788    2.7      345.      ]\n",
      " [ 28.77097  -79.66953    3.1      345.      ]\n",
      " [ 28.731968 -79.65838    2.8      346.      ]\n",
      " [ 28.70505  -79.65208    2.7      348.      ]\n",
      " [ 28.69665  -79.65053    2.9      351.      ]\n",
      " [ 28.68127  -79.64818    2.9      356.      ]\n",
      " [ 28.5732   -79.65772    3.2      358.      ]]\n",
      "[[ 28.599     -80.09809     2.617356  339.8919   ]\n",
      " [ 28.757193  -79.52089     2.5393558 343.73224  ]\n",
      " [ 28.868114  -79.259415    2.6267323 346.73233  ]\n",
      " [ 28.85906   -79.242035    2.5771031 346.72382  ]\n",
      " [ 28.634882  -79.886536    2.6227968 342.13806  ]\n",
      " [ 28.95586   -78.73142     2.541552  345.50787  ]\n",
      " [ 28.92229   -79.107605    2.5660608 347.18872  ]\n",
      " [ 28.831623  -79.30423     2.5567162 345.81754  ]\n",
      " [ 28.877045  -79.12519     2.5211217 347.37643  ]\n",
      " [ 28.847391  -79.41558     2.6342692 345.12433  ]\n",
      " [ 28.933502  -79.18832     2.4379828 347.65207  ]\n",
      " [ 28.704782  -79.903656    2.618457  341.88766  ]\n",
      " [ 28.854883  -79.240265    2.6308599 346.88382  ]\n",
      " [ 28.885386  -79.23484     2.4856977 346.89694  ]\n",
      " [ 28.686638  -79.56271     2.678881  344.27612  ]\n",
      " [ 28.796268  -79.46167     2.562071  345.42316  ]\n",
      " [ 28.805565  -79.43091     2.4241276 345.67258  ]\n",
      " [ 28.6378    -79.79467     2.500155  342.27234  ]\n",
      " [ 28.69002   -79.691765    2.453809  343.3037   ]\n",
      " [ 28.904774  -79.15017     2.5805979 347.4028   ]]\n",
      "LAT:  mean_absolute_error: 0.11770732700824738, mean_squared_error: 0.024131711572408676\n",
      "LON:  mean_absolute_error: 0.31294387578964233, mean_squared_error: 0.145401731133461\n",
      "SOG:  mean_absolute_error: 0.3230588138103485, mean_squared_error: 0.13270840048789978\n",
      "Test Done----\n",
      "number of training batch:  4\n",
      "Training----\n",
      "Train Done----\n",
      "Testing----\n",
      "[[ 32.55507   -91.10125     3.1       346.6      ]\n",
      " [ 32.48499   -91.11828     4.2       351.4      ]\n",
      " [ 32.45823   -91.0928      2.8       310.2      ]\n",
      " [ 32.431755  -91.02113     3.2       283.       ]\n",
      " [ 32.42941   -91.01625     2.8       296.4      ]\n",
      " [ 32.621635  -91.15007     4.        344.9      ]\n",
      " [ 32.941     -91.07272     1.        325.3      ]\n",
      " [ 32.41871   -90.99901     3.7999997 348.4      ]\n",
      " [ 32.45281   -91.08474     3.6       281.4      ]\n",
      " [ 32.438374  -91.04821     3.        290.4      ]\n",
      " [ 32.36235   -91.00532     5.8       354.9      ]\n",
      " [ 32.94314   -91.0742      2.3       341.1      ]\n",
      " [ 32.424862  -91.00741     3.4       285.7      ]\n",
      " [ 32.40306   -90.99218     3.3        11.3      ]\n",
      " [ 32.55019   -91.09988     3.3       349.6      ]\n",
      " [ 32.5941    -91.13232     3.9       317.4      ]\n",
      " [ 32.61736   -91.14881     3.7999997 347.1      ]\n",
      " [ 32.44932   -91.07607     3.7999997 307.1      ]\n",
      " [ 32.4719    -91.10809     1.8999999 301.1      ]\n",
      " [ 32.4327    -91.02481     3.2       284.8      ]]\n",
      "[[ 33.013515  -89.98688     4.480359  332.5871   ]\n",
      " [ 32.91099   -90.10777     4.2251496 328.9262   ]\n",
      " [ 32.74498   -90.44941     4.0075765 316.8363   ]\n",
      " [ 32.67184   -90.74021     3.955131  305.001    ]\n",
      " [ 32.689213  -90.68192     4.036097  310.53418  ]\n",
      " [ 32.833023  -90.1582      4.173059  330.52975  ]\n",
      " [ 33.013435  -89.942184    4.2554483 333.8905   ]\n",
      " [ 32.961212  -90.11556     4.2571893 330.05228  ]\n",
      " [ 32.562435  -90.95492     3.9141102 295.14087  ]\n",
      " [ 32.56385   -90.949715    3.9679914 295.26437  ]\n",
      " [ 32.820972  -90.22081     4.0378966 328.04764  ]\n",
      " [ 32.772434  -90.29567     3.913367  324.40884  ]\n",
      " [ 32.670834  -90.57374     3.8844838 313.98282  ]\n",
      " [ 31.952814  -92.68475     3.4912531 206.65352  ]\n",
      " [ 32.9695    -89.96828     4.2893457 334.44797  ]\n",
      " [ 32.761467  -90.4383      4.051351  314.91837  ]\n",
      " [ 32.961887  -90.13837     4.265508  328.2962   ]\n",
      " [ 32.635464  -90.721596    3.8939362 304.88797  ]\n",
      " [ 32.766384  -90.55023     4.0044456 311.63312  ]\n",
      " [ 32.457905  -91.042465    3.7225602 296.19232  ]]\n",
      "LAT:  mean_absolute_error: 0.31785067915916443, mean_squared_error: 0.1628371924161911\n",
      "LON:  mean_absolute_error: 0.7502936124801636, mean_squared_error: 0.9579418301582336\n",
      "SOG:  mean_absolute_error: 0.8776368498802185, mean_squared_error: 1.3078045845031738\n",
      "Test Done----\n",
      "number of training batch:  21\n",
      "Training----\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\ashdr\\miniconda3\\lib\\site-packages\\torch\\nn\\modules\\rnn.py:67: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.3 and num_layers=1\n",
      "  warnings.warn(\"dropout option adds dropout after all but last \"\n",
      "C:\\Users\\ashdr\\miniconda3\\lib\\site-packages\\torch\\nn\\modules\\rnn.py:67: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.3 and num_layers=1\n",
      "  warnings.warn(\"dropout option adds dropout after all but last \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iter: 100 loss：0.08209829032421112\n",
      "iter: 200 loss：0.053070373833179474\n",
      "iter: 300 loss：0.03042154386639595\n",
      "iter: 400 loss：0.017518293112516403\n",
      "Train Done----\n",
      "Testing----\n",
      "[[ 43.83012 -87.62647   8.4     188.8    ]\n",
      " [ 41.83387 -87.4802    8.4     167.7    ]\n",
      " [ 42.7181  -87.6981    8.6     197.1    ]\n",
      " [ 41.76577 -87.45985   8.3     163.5    ]\n",
      " [ 43.04578 -87.83457   8.4     179.2    ]\n",
      " [ 42.3692  -87.73287   8.5     177.5    ]\n",
      " [ 42.58627 -87.74897   8.7     196.9    ]\n",
      " [ 43.32833 -87.83285   8.      177.6    ]\n",
      " [ 42.37703 -87.73362   8.6     174.     ]\n",
      " [ 41.88592 -87.49763   8.3     167.7    ]\n",
      " [ 43.09595 -87.83428   8.4     177.5    ]\n",
      " [ 41.88855 -87.49838   8.3     168.6    ]\n",
      " [ 43.92475 -87.60192   8.3     189.00002]\n",
      " [ 43.9099  -87.60568   8.4     190.6    ]\n",
      " [ 43.5062  -87.74257   8.4     189.7    ]\n",
      " [ 43.8187  -87.62935   8.3     190.10002]\n",
      " [ 43.96405 -87.59228   8.4     188.60002]\n",
      " [ 41.79707 -87.46903   8.4     167.9    ]\n",
      " [ 43.7668  -87.6428    8.3     190.8    ]\n",
      " [ 43.34978 -87.82442   8.4     202.79999]]\n",
      "[[ 43.118793 -87.43242    8.579792 178.02579 ]\n",
      " [ 42.19349  -88.35165    9.48653  159.55104 ]\n",
      " [ 42.869225 -87.75532    8.739783 175.40765 ]\n",
      " [ 42.11153  -88.46763    9.531408 158.30464 ]\n",
      " [ 42.72492  -87.73978    8.887137 169.80437 ]\n",
      " [ 42.43068  -88.17171    9.269713 165.51404 ]\n",
      " [ 43.04488  -87.469795   8.613558 178.37498 ]\n",
      " [ 42.798958 -87.696106   8.687583 171.6896  ]\n",
      " [ 42.52887  -88.01795    9.209916 166.0915  ]\n",
      " [ 42.25004  -88.33207    9.446451 160.98047 ]\n",
      " [ 42.71766  -87.78823    8.89928  169.68852 ]\n",
      " [ 42.228306 -88.29355    9.416729 159.662   ]\n",
      " [ 43.218834 -87.3402     8.366546 179.51161 ]\n",
      " [ 43.134773 -87.439316   8.622863 177.88481 ]\n",
      " [ 43.157284 -87.39312    8.568274 179.01433 ]\n",
      " [ 43.132248 -87.35756    8.458423 178.63121 ]\n",
      " [ 43.250473 -87.2754     8.255434 180.95529 ]\n",
      " [ 42.16969  -88.36512    9.397921 159.9068  ]\n",
      " [ 43.220074 -87.203766   8.334828 179.26787 ]\n",
      " [ 43.41498  -87.06203    8.240974 185.22769 ]]\n",
      "LAT:  mean_absolute_error: 0.4022657871246338, mean_squared_error: 0.24841859936714172\n",
      "LON:  mean_absolute_error: 0.5140209197998047, mean_squared_error: 0.3970092535018921\n",
      "SOG:  mean_absolute_error: 0.7575916647911072, mean_squared_error: 0.9274017810821533\n",
      "Test Done----\n"
     ]
    }
   ],
   "source": [
    "# loss=nn.L1Loss(reduction='sum')\n",
    "X,y,labels = shuffle(X,y,labels,random_state=711)\n",
    "\n",
    "X_train,y_train,labels_train,X_test,y_test,labels_test = split_data(X,y,labels,ratio)\n",
    "train_data_for_each_net = []\n",
    "train_predict_for_each_net = []\n",
    "test_data_for_each_net = []\n",
    "test_predict_for_each_net = []\n",
    "for i in range(cluster_num):\n",
    "    train_data_for_each_net.append(list())\n",
    "    train_predict_for_each_net.append(list())\n",
    "    test_data_for_each_net.append(list())\n",
    "    test_predict_for_each_net.append(list())\n",
    "\n",
    "num1=0\n",
    "num2=0\n",
    "for i in range(len(X_train)):\n",
    "    id = labels_train[i]\n",
    "    if id != -1:\n",
    "        train_data_for_each_net[id].append(X_train[i])\n",
    "        train_predict_for_each_net[id].append(y_train[i])\n",
    "    else:\n",
    "        num1+=1\n",
    "for i in range(len(X_test)):\n",
    "    id = labels_test[i]\n",
    "    if id != -1:\n",
    "        test_data_for_each_net[id].append(X_test[i])\n",
    "        test_predict_for_each_net[id].append(y_test[i])\n",
    "    else:\n",
    "        num2+=1\n",
    "print(f\"unuse train_data:{num1} , unuse test_data:{num2}\",)\n",
    "for i in range(cluster_num):\n",
    "    if len(train_data_for_each_net[i])<100 or len(test_data_for_each_net[i])==0:\n",
    "        continue\n",
    "    train_dataset,test_dataset = get_dataset(torch.stack(train_data_for_each_net[i]),torch.stack(train_predict_for_each_net[i]),torch.stack(test_data_for_each_net[i]), torch.stack(test_predict_for_each_net[i]))\n",
    "    train_loader = DataLoader(dataset=train_dataset,batch_size=batch_size,shuffle=True,drop_last=True)\n",
    "    test_loader = DataLoader(dataset=test_dataset,batch_size=batch_size,shuffle=True,drop_last=True)  #batch_size=32   [32,10,5]\n",
    "    net = SmallNet(train_iter=train_loader,test_data_X=test_data_for_each_net[i],test_data_y=test_predict_for_each_net[i])\n",
    "    net.train()\n",
    "    net.test()"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [],
   "metadata": {
    "collapsed": false
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
